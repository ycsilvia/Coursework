{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Introduction and Multivariate Normals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why sampling random vectors is hard\n",
    "So for, our primary focus has been sampling independent sets of random numbers. However, let's suppose we are interested in sampling instead from a random vector in Rd, which potentially has an arbitrary joint pdf (i.e., it may not be independent). \n",
    "\n",
    "Obviously, if we are assuming they are independent, we can use all of our previous techniques once we have factored the pdf/pmf/cdf appropriately.\n",
    "\n",
    "However, the first thing we will notice is that for a random vector; there is generally no such thing as an inverse transform. the first thing we will notice is that for a random vector; there is generally no such thing as an inverse transform. Let's suppose it has multidimensional cdf F(x1,...,xd). Then the equation F(x1,...,xn)=u for u∈(0,1) does not have a unique solution. This will typically define some sort of d−1 dimensional curve in Rd. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What, then, about rejection sampling?\n",
    "\n",
    "Well, the tricky part with rejection sampling in higher dimensions is that (1) we need to be able to sample from a proposal density in Rd in the first place, which we don't have a good method for and (2) generally speaking, the values of c will almost always get very large (the curse of dimensionality). \n",
    "\n",
    "In this section, we'll start with probably the simplest distribution to sample from: the multivariate normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the multivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.58828805 0.76275282 0.04895913]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import cholesky, eig\n",
    "\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "## Part 1\n",
    "\n",
    "n = 1000\n",
    "\n",
    "mu = np.array([0.25, 0.5, -1])\n",
    "Sigma = [[2., 0.9, 1.2],\n",
    "         [0.9, 1.4, 1.],\n",
    "         [1.2 , 1., 1.]]\n",
    "Sigma = np.array(Sigma) \n",
    "\n",
    "# Eigenvalue decomposition of the covariance matrix\n",
    "w,v = eig(Sigma)\n",
    "print(w)\n",
    "# Cholesky decomposition\n",
    "c = cholesky(Sigma)\n",
    "\n",
    "Z = norm.rvs(size=(3, n))\n",
    "\n",
    "X = np.zeros((3, n))\n",
    "X = c.dot(Z)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "mvn = multivariate_normal(mean=mu, cov=Sigma)\n",
    "X = mvn.rvs(size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Copulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example\n",
    "Suppose have a data set X1,...,Xn of d-dimensional vectors, and we wish to generate more such samples. \n",
    "\n",
    "Perhaps these are the arrival times of a customer, how long it takes them to place an order, how long it takes for their order to be made once they are in service, and how much they spent. It makes sense that the last three of these would be correlated - the longer someone spends ordering, the more likely they are to have ordered more items, and thereby spent more money. Perhaps customers arriving at certain times of day might have longer orders - dinner patrons at a restaurant might order multiple courses for multiple people, whereas morning patrons may just be buying a coffee.\n",
    "\n",
    "In this case, the marginal distributions are likely non-normal, but perhaps they have a joint structure that is similar to a normal distribution. So, if we change these into something that is (approximately) multivariate normal, we can estimate their covariance matrix and work backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the gaussian copula\n",
    "The gaussian copulas with parameter Σ (positive definite matrix) is the copula of a multivariate normal with μ=0 (this is actually not important) and covariance Σ where Σi,i =1. We can sample (U1,...Ud) as follows:\n",
    "\n",
    "- Generate a sample (X1,...Xd)∼N(0,Σ) using the previously discussed method with the Cholesky decomposition\n",
    "\n",
    "- Let (U1,...Ud)∼(Φ(X1),...,Φ(Xd))\n",
    "\n",
    "We can also use the scipy.stats.multivariate_normal object with method .rvs() to sample this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Copula\n",
    "\n",
    "### Part I\n",
    "\n",
    "Write a function that generates random vectors from the Gaussian Copula given a matrix Sigma.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Suppose $X_i$ is a random vector with d=3, a Gaussian Copula, and the marginals are $exp(1)$. Let $Y_i = \\sum_j=1^3 X_i(j)$\n",
    "\n",
    "Calculate the variance of Y_i when $\\Sigma=I_3$ (the identity matrix) or when it is the matrix given below. Calculate the covariance matrix of $X_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.057390168962806\n",
      "7.159829864691528\n",
      "[[ 0.97720046  0.03737646 -0.01648467]\n",
      " [ 0.03737646  0.99180865  0.0178858 ]\n",
      " [-0.01648467  0.0178858   1.01388633]]\n",
      "[[0.99119078 0.78858536 0.59409564]\n",
      " [0.78858536 1.02239611 0.68206927]\n",
      " [0.59409564 0.68206927 1.02390941]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm, expon\n",
    "\n",
    "# part I\n",
    "\n",
    "def gaussian_copula(Sigma, n):\n",
    "    d = len(np.diag(Sigma))\n",
    "    X = multivariate_normal.rvs(cov=Sigma, size=n)\n",
    "    C = np.zeros((n,d))\n",
    "    for j in range(0, d):\n",
    "        C[:,j] = norm.cdf(X[:,j], scale=np.sqrt(Sigma[j,j]))\n",
    "    return C.T\n",
    "\n",
    "# part II\n",
    "\n",
    "C0 = gaussian_copula(np.eye(3), 1000)\n",
    "X0 = expon.ppf(C0)\n",
    "Y0 = np.sum(X0, axis=0)\n",
    "\n",
    "Sigma = np.array([[1, 0.8, 0.6,], [0.8, 1, 0.7], [0.6, 0.7, 1]])\n",
    "C1 = gaussian_copula(Sigma, 1000)\n",
    "X1 = expon.ppf(C1)\n",
    "Y1 = np.sum(X1, axis=0)\n",
    "\n",
    "print(np.var(Y0))\n",
    "print(np.var(Y1))\n",
    "\n",
    "print(np.cov(X0))\n",
    "print(np.cov(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the t-copula\n",
    "A common downside of the gaussian copula is that tail dependence is very weak. Let's consider the following toy example - let's suppose AAPL and SPX have a joint multivariate normal distribution. Let's say they each have a standard deviation of daily returns of 1%, and their correlation is quite high 0.7, and let's say their expected return is something like 0. If the SPX is down 1% today, we would expect AAPL to be down about 0.7% on average, which seems reasonable. \n",
    "\n",
    "But what if the SPX crashed down 50% tomorrow? If they had a gaussian copula, then we would expect AAPL to only be down about 35 - but in reality, if there is a crash, we essentially expect correlation to go to one - i.e. AAPL should be down about 50% as well. \n",
    "\n",
    "The t-copula is an alternate copula with much stronger tail dependences. It is, as you might expect, the copula for a multivariate t-distribution. What exactly is a multivariate t-distribution? \n",
    "\n",
    "This will induce fatter tails in the marginals and greater tail dependencies because outliers occur because of small values of Y.\n",
    "\n",
    "We can use the following algorithm to sample from the tcopula with parameters Σ and n:\n",
    "- Sample Z∼N(0,Σ) and Y χn^2\n",
    "- Let $X~Z/\\sqrt(Y/n)$\n",
    "- Return U=(F(X1),...,F(Xd)) where F is the t cdf\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-copula\n",
    "\n",
    "## Part I\n",
    "\n",
    "Write code that samples from a t-copula.\n",
    "\n",
    "## Part II\n",
    "\n",
    "Suppose $X_i$ is a random vector with d=3, and the marginals are $exp(1)$. Let $Y_i = \\sum_j=1^3 X_i(j)$\n",
    "\n",
    "Let Sigma be the matrix below. Estimate the probability that $Y_i>10$ if we have a gaussian copula, and if we have a t-copula with 4 degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0055\n",
      "0.0056\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2, expon, multivariate_normal, norm, t\n",
    "\n",
    "def gaussian_copula(Sigma, n):\n",
    "    d = len(np.diag(Sigma))\n",
    "    X = multivariate_normal.rvs(cov=Sigma, size=n)\n",
    "    C = np.zeros((n,d))\n",
    "    for j in range(0, d):\n",
    "        C[:,j] = norm.cdf(X[:,j], scale=np.sqrt(Sigma[j,j]))\n",
    "    return C.T\n",
    "\n",
    "def t_copula(Sigma, df, n):\n",
    "    d = len(np.diag(Sigma))\n",
    "    X = multivariate_normal.rvs(cov=Sigma, size=n)\n",
    "    v = chi2.rvs(df, size=n)\n",
    "    T = X\n",
    "    for i, x in enumerate(X):\n",
    "        T[i] = x / np.sqrt(v[i] / df)\n",
    "    C = np.zeros((n,d))\n",
    "    for j in range(0, d):\n",
    "        C[:,j] = t.cdf(T[:,j], df=df, scale=np.sqrt(Sigma[j,j]))\n",
    "    return C.T\n",
    "\n",
    "Sigma = np.array([[1, 0.8], [0.8, 1.0]])\n",
    "\n",
    "n = 10_000\n",
    "\n",
    "C0 = gaussian_copula(Sigma, n)\n",
    "X0 = expon.ppf(C0)\n",
    "Y0 = np.sum(X0, axis=0)\n",
    "\n",
    "C1 = t_copula(Sigma, 1, n)\n",
    "X1 = expon.ppf(C1)\n",
    "Y1 = np.sum(X1, axis=0)\n",
    "\n",
    "print(np.mean(Y0 > 10))\n",
    "print(np.mean(Y1 > 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Copulas\n",
    "There are of course, infinitely many other copulas (every possible joint distribution has a copula). There are relatively few other copulas, though, that have neat parametric forms and can be simulated from in a simple manner.\n",
    "\n",
    "The most common types of these are what are called Archmidean copulas; however, working with them is beyond the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Markov Chain Monte Carlo\n",
    "\n",
    "## Introduction\n",
    "While the copulas can be useful for many applications, ultimately they only work exactly if the distribution we are looking to sample from happens to have a copula we know how to work with.\n",
    "\n",
    "Let's return to the basic question: how do we sample from a d-sample dimensional density f(x1,...,xd)?\n",
    "\n",
    "Markov Chain Monte Carlo gives us a way of generating samples that closely approximate this distribution. Moreover, it turns out that we often don't even have to know the exact density; we just need to know it up to a constant of proportionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note about Bayesian Statistics\n",
    "This kind of problem frequently arises in Bayesian statistics. The Bayesian approach works as follows. Our data X1,...,Xn is assumed to come from a parametric model $P(X_1,...,X_n|\\Theta)$. We have some parameters Θ about which we have some prior information, which can be expressed in terms of a prior distribution $P(\\Theta)$ - this could be the outcome of some previous experiments, some knowledge that the investigator has, or just the application of certain heuristics. Given the data and the prior, we can apply Bayes rule to get the posterior distribution for Θ - our estimates will be the expectation or mode or medians of this distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic idea\n",
    "As you presumably know, a Markov chain is a sequence of random variables X1,X2,... with shared support S, a starting probability distribution p1 for X1 and a transition kernel p(x∣y)=p(x∣Xi−1=y) i.e. the conditional density (or mass function) of Xi∣Xi−1=y. Notably, the transition kernel depends only on the value of the previous element in the sequence; the history of the path before that does not matter.\n",
    "\n",
    "Generally speaking, the unconditional distribution pj of Xj will not be the same as pi of Xi; however, given some technical conditions, there is a choice of p1 - we'll call it π that causes the Markov chain to be stationary; that is, every random variable has the same distribution π.\n",
    "\n",
    "Moreover, regardless of the choice of p1,limt→∞pi=π.\n",
    "\n",
    "So the idea for MCMC is that we are going to choose a transition kernel on our support in a clever manner, such that the limiting/stationary distribution is f(x1,...,xn). Then, regardless of how we X1, Xj for j large will be have approximately distribution f(x1,...,xn), and our sequence will be a (correlated sample) from our target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cholesky decomposition L\n",
    "L = np.array([[1, 0], \n",
    "              [1, 1]])\n",
    "\n",
    "# Calculating the covariance matrix Sigma\n",
    "Sigma = np.dot(L, L.T)\n",
    "\n",
    "# Extracting the variances and covariance\n",
    "var_X = Sigma[0, 0]  # Variance of X\n",
    "var_Y = Sigma[1, 1]  # Variance of Y\n",
    "cov_XY = Sigma[0, 1]  # Covariance between X and Y\n",
    "\n",
    "# Calculating the correlation\n",
    "correlation_XY = cov_XY / np.sqrt(var_X * var_Y)\n",
    "correlation_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.sqrt(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
