{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Decision Optimization with hyperopt\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lecture, we give an introduction to decision optimization \n",
    "\n",
    "Course objective:\n",
    "\n",
    "- Understand how to use simulation for decision optimization\n",
    "\n",
    "- Understand the basic idea behind the following search algorithms:\n",
    "\n",
    "    - Grid search\n",
    "\n",
    "    - Random search\n",
    "\n",
    "    - Tree-structured Parzen estimator. \n",
    "\n",
    "- how to use hyperopt to perform search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision-making optimization\n",
    "\n",
    "To high-light behind using simulation for decision-making optimization, let's go back to a single-server queueing system. \n",
    "\n",
    "Assume that we have a single-server queueing system with the following elements:\n",
    "\n",
    "- The arrival process follows homogeneous Poisson process with ($\\lambda_\\text{a}$ =7)\n",
    "\n",
    "- Service time follows t=0.2\n",
    "\n",
    "- First come first sequence \n",
    "\n",
    "- Unlimited queues\n",
    "\n",
    "- The shop opens for 8 units of time. At open for 8 hours,  the store finishes serving the current customer and turns away all the other customers. \n",
    "\n",
    "For a customer we finish servicing, the business gets revenue of 4. At the same time, the company also cares about customers who cannot receive the service as they might not come back in the future. The company converts the loss of customers to a loss of revenue of 2. The company wants to maximize 4s−2i between 0 and 8, where s is the number of customers served, and i is the number of customers lost.\n",
    "\n",
    "The company is trying to look for the following options:\n",
    "\n",
    "- Limit the capacity of queue to n. \n",
    "\n",
    "Converting the question to optimization question:\n",
    "\n",
    "Objective function: 4s−2i with no constraints. \n",
    "\n",
    "Constraints: n=1,2,3,...\n",
    "\n",
    "How is this optimization question different from the regular optimization problems we saw from an optimization course? s and i are both complicated functions of system input. In optimization, this is called black-box objective function. As the objective function itself is random, we need to choose a method that is suitable for the optimization in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing decision variables\n",
    "\n",
    "Let's first look at two simple algorithms: Grid search and Random search \n",
    "\n",
    "- Grid search \n",
    "\n",
    "    - We divide our decision variable into different segments\n",
    "\n",
    "    - We will try out a value in each segment and compute the objective value\n",
    "\n",
    "    - Choose the best value to optimize the objective function\n",
    "\n",
    "- Random search \n",
    "\n",
    "    - Repeat the following N times\n",
    "\n",
    "        - Randomly sample a value from the whole space and compute the objective value \n",
    "\n",
    "    - Choose the best value to optimize the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the methods is called Tree-structured Parzen estimators (TPE).\n",
    "\n",
    "TPE model is to start from random search. We sample from x from its space and compute the corresponding objective function value y. Once we get enough samples, the model will then split the outcomes using y∗ as the cutoff point.  Without losing generality, assume that we are performing a minimization problem. Then samples with values y < y∗ are all better outcomes, while sample with values y > y∗ are all worse outcomes.\n",
    "\n",
    "Based on the outcomes, we can now construct two distributions of x based on these two sets of samples: One is the distribution constructed using the good sample values x (those leading to small y values), while the other one is the distribution constructed using the bad sample values x (those leading to large y values). More formally, let's define them as \n",
    " \n",
    "$$\\begin{equation}\n",
    "p(x|y) =\n",
    "   \\left\\{\\begin{array}{lr}\n",
    "       l(x), & if y < y* \\\\\n",
    "       g(x), & otherwise \n",
    "    \\end{array}\\right.\n",
    " \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One easy way to construct these distributions is by using kernel method (for example Gaussian kernel). This can be done easily using scipy.stats.gaussian_kde() . The math behind how to find such distribution is not required. For those who are interested, please refer to this page (https://en.wikipedia.org/wiki/Kernel_density_estimation). \n",
    "\n",
    "It can be proved (page 4, reading) that the next sampling point should be the value that makes $l(x)/g(x)$ largest. The intuition is that the next sample should be the one that are very likely to happen in l(x) but less likely to happen at g(x). In addition, to make sure we still get the exploration going, people usually generate n samples from l distribution and choose the ones with the highest $l(x)/g(x)$ to explore. \n",
    "\n",
    "The following picture shows a visual comparison between grid search and random search. \n",
    "\n",
    "https://medium.com/criteo-engineering/hyper-parameter-optimization-algorithms-2fe447525903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use hyperopt for search\n",
    "\n",
    "We can use hyperopt to perform TPE. (https://github.com/hyperopt/hyperopt/wiki/FMin)\n",
    "\n",
    "To perform optimization, we need to define two main components:\n",
    "\n",
    "- Objective function. \n",
    "\n",
    "    - This is the function that we want to minimize or maximize\n",
    "\n",
    "    - In the previous example, our function can be defined as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    return -(3*x-3)**3+(x+20)**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use hyperopt.fmin to perform minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hyperopt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mhyperopt\u001b[49m\u001b[38;5;241m.\u001b[39mfmin(fn\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[0;32m      2\u001b[0m     space\u001b[38;5;241m=\u001b[39mhyperopt\u001b[38;5;241m.\u001b[39mhp\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m      3\u001b[0m     algo\u001b[38;5;241m=\u001b[39mhyperopt\u001b[38;5;241m.\u001b[39mtpe\u001b[38;5;241m.\u001b[39msuggest,\n\u001b[0;32m      4\u001b[0m     max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hyperopt' is not defined"
     ]
    }
   ],
   "source": [
    "best = hyperopt.fmin(fn=objective,\n",
    "    space=hyperopt.hp.uniform('x', -10, 10),\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fn gives the callable. If the task is maximization, then the objective function should return -1 times the objective value. \n",
    "\n",
    "- space defines the distribution of the search space \n",
    "\n",
    "    - Access here for a list of commonly used space options\n",
    "\n",
    "    - For discrete uniform, use hyperopt.hp.quniform('x', lower, upper, q)\n",
    "\n",
    "        - For example, if -20,-10,0,10, 20. Use hyperopt.hp.quniform('x', -20, 20, 10)\n",
    "\n",
    "- algo gives the algorithm used for search. \n",
    "\n",
    "    - tpe.suggest uses the tree-structured Parzen estimator.\n",
    "\n",
    "    - rand.suggest uses random search\n",
    "\n",
    "- max_evals specifies how many evaluations should be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:10<00:00, 29.18trial/s, best loss: 263.25136559835096]\n",
      "{'x': -4.507111801227611, 'y': -3.0}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt \n",
    "def objective(params):\n",
    "  x=params[0]\n",
    "  y=params[1]\n",
    "  return -1*(-(x+y+12)**3-y**4+x**3)\n",
    "\n",
    "trials=hyperopt.Trials()\n",
    "best = hyperopt.fmin(fn=objective,\n",
    "    space=[hyperopt.hp.uniform('x', -10, 10),hyperopt.hp.quniform('y', -5, 5,1)],\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=300,\n",
    "    trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiple parameter search and Track history\n",
    "\n",
    "In this example, we study how to maximize the following question\n",
    "\n",
    "- $-(x+y+12)**3 - y**4 + x**3$\n",
    "    - where -10 < x < 10, and y = -10, -9, -8, ..., 9, 10\n",
    "    \n",
    "Again, let's define our objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "  x=params[0]\n",
    "  y=params[1]\n",
    "  return -1*(-(x+y+12)**3-y**4+x**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two important things:\n",
    "\n",
    "- This function can only have one argument. As a result, we pass a list of parameters. The first parameter is assigned to x, while the second parameter is assigned to y.\n",
    "\n",
    "- Since we are performing maximization. We multiple the objective value by minus 1 to convert this question to a minimization question. \n",
    "\n",
    "For the minimization step, we specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = hyperopt.fmin(fn=objective,\n",
    "    space=[hyperopt.hp.uniform('x', -10, 10),hyperopt.hp.quniform('y', -5, 5,1)],\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=300)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt \n",
    "def objective(params):\n",
    "  x=params[0]\n",
    "  y=params[1]\n",
    "  return -1*(-(x+y+12)**3-y**4+x**3)\n",
    "\n",
    "best = hyperopt.fmin(fn=objective,\n",
    "    space=[hyperopt.hp.uniform('x', -10, 10),hyperopt.hp.quniform('y', -5, 5,1)],\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=300)\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, if we want to track the optimization history, we can use hyperopt.Trials(). The modification is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(fn=objective,\n",
    "    space=[hyperopt.hp.uniform('x', -10, 10),hyperopt.hp.quniform('y', -5, 5,1)],\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=300,\n",
    "    trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trials objective is a dictionary that stores all the optimization histories.\n",
    "\n",
    " For example, trials.vals gives the history of x and y values. trial.results gives the minimization outcome from each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:03<00:00, 80.79trial/s, best loss: 263.5108900840554] \n",
      "{'x': [3.5935744471651105, 8.635121124209817, -7.916771691620834, -8.03184335652681, -8.601394099045551, 1.8495790911143732, 5.123260693700082, -2.1565045799806644, -9.883881344882731, 3.6636499957501716, -5.434435460356721, 7.678439867339822, -1.1237592101654315, -8.261562174787038, 6.108786149126647, 9.783276986052872, 6.1724329733580205, 8.282470238957455, -8.298733555074282, 9.084173500793597, -3.555685305788072, -4.725546450265034, -5.3084372917219005, -4.83955263717494, -5.808420092849731, -0.059399956501061446, -3.145198314211906, -6.4900174157617005, 1.2250282286175516, -4.326866676125135, -1.9428326446344633, -9.926117123987687, 0.16848931043925575, -6.944508244406338, -6.958847425061739, -3.397535531501685, 2.5339088520936794, -1.8963349334783355, -9.855028408612228, -0.36420329686606223, -4.797456440657934, -9.025659766095103, -7.432903699685925, -6.2577491451511, 3.7226105668359826, -0.9612946502907498, 1.1977380725052686, -2.9025151233017272, -9.048586933471618, -4.164131833667398, -2.5732217832525026, -5.1853850160349495, -1.4047895317351284, 2.848642808119986, -7.627034850893725, 4.846367956511626, -5.856162820999158, -4.344168480852302, -9.044934472794473, 0.7092011032960581, -3.935638965906968, -0.6007521624353318, 6.807238679921609, -1.6022407933118452, -8.051417985346056, -2.612443664193278, -4.834600896246984, -4.744641981172128, -3.8279232057805137, -6.672819748755799, -5.731636516846336, -4.407848402785484, -3.3382604022448876, -8.618849523609484, -7.259750674191836, -2.315297148175915, -5.415335419115929, 1.906214280619544, -9.482856843742466, -6.23693838364931, -4.960667700171105, -3.187275642728577, -0.9632548030233248, 0.4399920176492875, -6.013576022048552, -6.975527556132058, -7.826239064116474, -2.0185758676568155, -4.40558724519502, -0.151414314230764, -8.641773579530513, -3.686989653738512, 4.553472398810193, -0.6940213742468151, 1.3188740366516503, -1.2988503051513236, -2.6915194332988155, -6.696005144445453, 3.0789117578990948, -1.6541695886303662, -9.999748742070413, -5.597975780410977, -8.347305434465358, -7.295701476197792, 9.617957192919238, -9.502568697620237, -4.112738172683886, -3.031755457319221, -5.193769591907575, 1.833266668494307, -4.817447323227699, -2.412847532985768, 0.32677998218959203, -6.414102708980407, -7.697798480534363, 5.65532891602106, -3.4981339534099525, -6.013281950755582, 4.07116373602943, 0.9614281217121619, 7.780357695815505, -4.547262414158344, -9.53333909028795, -1.9306477519549325, -3.8876968943757877, -0.43819132066116584, -7.070228206638988, -8.212377361520984, -5.3729897927580605, -8.98547542491075, 1.9907046037301983, -2.8512814963790305, -1.0146078397967373, -1.6347485004544946, 0.06658969220412736, -6.623311973264931, -5.05386424395874, -3.477679929925435, -4.291602546949704, -7.5250133638181325, -2.23964571181609, -5.104699990848052, -5.75386144420066, -8.645234575910575, -6.2953052694868905, -4.670889828078349, -3.035181875600799, -5.619140428853437, -3.9250333596656004, -6.788948981876238, -2.672527646912587, -8.111566988931482, -6.080181512183938, -5.146332600336825, -3.3393197154309773, -0.6526341915993257, -4.239603227436702, -1.3197894666157421, -7.245505648885701, -2.1537698723842187, -5.149842655328906, -3.791264824584185, -9.182908905583016, -7.756062573607775, 0.6127612654057524, -5.485225408147933, -6.484349018037191, -4.550511150206183, 2.429997447543527, -0.10483850417816765, -9.698335671276197, 6.7368526230677155, -8.524596280520042, -2.4639246798482217, -1.6178695429340486, -6.89817553625463, -9.980318960925032, 1.3636672685298248, -4.965992484944016, -3.4820213648954095, -5.848951345129221, -4.043568044976665, -2.9648788151258105, -7.809305489443516, -7.335037041608736, 3.2253715430541954, -6.186957969383306, -9.031390155658537, -0.8873186242898541, -2.064852302929237, -5.28394520531528, -4.645931844681433, 8.68133704678441, -4.580122694189745, -3.1639034476964585, -3.7545165760487316, -1.2799050463471544, -1.7798903911189345, -2.564317058260097, -8.131086047932918, -6.397832199321885, -4.276720459156298, 1.581127529656821, -0.3925705495900944, 0.5601974671819967, -4.257671006332801, 1.0394872838292621, -5.747636237055522, 0.12020114680711647, -7.071790107303455, 4.097762204914009, -2.787885783452446, -4.716487646132228, 2.5880002457805213, -1.0988871120802282, -3.5358369153507265, -6.722716298174214, -1.9851484012447576, -8.783192070924107, -7.520188633933737, -5.566997944195369, -4.029373090720442, -5.949737275257652, -3.0889222369190077, -2.3788229456262644, -0.7560432347838755, -9.381426370997348, -4.451129875915712, -1.5516761660503646, -8.362524692174912, 5.23331579602718, -3.26156897288274, -0.2069042967734429, -6.453302613407182, -3.749137723809353, -5.011092004698997, 0.8707560356703237, -7.0211534960170345, -7.9771722143980215, -6.103212153057552, -9.940531558799279, -2.7572196162871787, -2.228676010949091, 0.2988517329652485, -5.561788565348215, -4.254868711171563, -7.487661364426284, -1.199558763964426, -4.666166429850672, -5.219520934935632, -4.794476892284969, -3.4809357275585078, -1.8139219887420306, -6.8435488536834335, -0.6234754807630658, 2.1843372817757043, -4.029842313261912, -4.706670319563659, -5.974648611672839, -2.980771509193068, -4.598298503069762, -6.4359649143187525, -4.73803428552315, -5.425074555606604, -3.6718989948183536, -7.263323707297316, -2.5721428117179985, -5.778820510822627, -7.779074774342584, -4.514673037787193, -5.086366113418908, -3.230278931863717, -6.618674778537535, -8.447824313948253, -3.832280865998214, -2.2117973640246604, -1.5044632980058616, -6.062336716164757, -5.30888441580902, -2.739856521383128, -8.918108287352261, -4.220663277583596, -4.827855986122668, -6.957125260510008, -3.331392188463507, -1.9262818292450117, -7.632193322695159, -6.317380535226004, -5.678436928852411, -3.6905838790851826, 9.828738870533488, -0.2656494079341165, -1.090332253806754, -4.369942474620762, -8.081429791896277, -2.3731523695306733, -9.410903030533103, -2.9373099267309533, -5.291699751858153, -6.637124245728293], 'y': [1.0, 5.0, -4.0, -2.0, -3.0, -0.0, 1.0, -4.0, -0.0, -0.0, -4.0, 3.0, 2.0, -4.0, -4.0, -2.0, -2.0, -2.0, -5.0, -2.0, -5.0, -3.0, -3.0, -1.0, -3.0, -1.0, -3.0, -1.0, -5.0, 4.0, 1.0, -3.0, -1.0, -3.0, -5.0, -3.0, -4.0, 1.0, 2.0, 0.0, -1.0, -2.0, 5.0, -4.0, 2.0, 1.0, -3.0, -4.0, -5.0, -0.0, -2.0, -5.0, -4.0, 4.0, 0.0, -2.0, -1.0, -3.0, -4.0, -2.0, 3.0, -1.0, -5.0, -3.0, -4.0, -3.0, -2.0, -2.0, -2.0, -3.0, -1.0, -2.0, -1.0, -0.0, -3.0, -4.0, -5.0, -3.0, -2.0, -1.0, -4.0, -5.0, -3.0, -0.0, -2.0, 1.0, -3.0, -4.0, -1.0, -2.0, -4.0, -5.0, 1.0, -3.0, -1.0, -2.0, -0.0, -4.0, -3.0, 3.0, -1.0, -5.0, -2.0, -3.0, -0.0, 2.0, 5.0, 1.0, -4.0, 4.0, -2.0, -1.0, -5.0, -3.0, -4.0, -2.0, 0.0, -3.0, -1.0, -4.0, -2.0, -5.0, -3.0, -4.0, -5.0, -1.0, -2.0, -3.0, -0.0, -1.0, -2.0, -4.0, -5.0, 2.0, -3.0, -3.0, -2.0, -1.0, 1.0, -4.0, -2.0, -0.0, -5.0, -3.0, -4.0, -2.0, -1.0, -2.0, -3.0, -2.0, -3.0, -1.0, -3.0, -2.0, -4.0, -2.0, -1.0, 0.0, -3.0, -4.0, -2.0, -3.0, -2.0, -1.0, -5.0, -3.0, -4.0, 1.0, 0.0, -2.0, -1.0, -4.0, -2.0, 2.0, -3.0, -1.0, -5.0, -3.0, 1.0, -4.0, -5.0, -0.0, -2.0, -1.0, -2.0, -3.0, -4.0, -2.0, -5.0, 4.0, -4.0, -3.0, -3.0, -5.0, -4.0, 3.0, -3.0, -4.0, -5.0, -3.0, -4.0, -3.0, -5.0, -4.0, -5.0, -3.0, -3.0, -4.0, -4.0, 5.0, -5.0, -3.0, -4.0, -1.0, -2.0, -3.0, -1.0, -5.0, -3.0, -2.0, 0.0, -3.0, 3.0, -5.0, -4.0, -2.0, -2.0, -4.0, -1.0, -4.0, -5.0, -3.0, -2.0, 2.0, -0.0, -1.0, -3.0, -4.0, -3.0, -5.0, -2.0, -3.0, 1.0, -4.0, -5.0, -1.0, 4.0, -2.0, -3.0, -3.0, -0.0, -4.0, -2.0, -3.0, -1.0, -4.0, -5.0, -3.0, -3.0, -2.0, -3.0, -2.0, -3.0, -4.0, -4.0, -3.0, -2.0, -3.0, -2.0, -4.0, -3.0, -4.0, -3.0, -2.0, -5.0, -4.0, -3.0, -2.0, -1.0, -5.0, -4.0, -3.0, -2.0, -3.0, -2.0, -1.0, -4.0, -5.0, -3.0, -2.0, -4.0, -5.0, -3.0, -1.0, -4.0, -3.0, -2.0, -3.0, -4.0, -3.0]}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt \n",
    "def objective(params):\n",
    "  x=params[0]\n",
    "  y=params[1]\n",
    "  return -1*(-(x+y+12)**3-y**4+x**3)\n",
    "\n",
    "trials=hyperopt.Trials()\n",
    "best = hyperopt.fmin(fn=objective,\n",
    "    space=[hyperopt.hp.uniform('x', -10, 10),hyperopt.hp.quniform('y', -5, 5,1)],\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=300,\n",
    "    trials=trials)\n",
    " \n",
    "print(trials.vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import simpy\n",
    "import matplotlib.pyplot as plt \n",
    "import hyperopt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps. Simpy\n",
    "\n",
    "simpy.Environment.process - 添加仿真进程\n",
    "\n",
    "simpy.Environment.event - 创建事件\n",
    "\n",
    "simpy.Environment.timeout - 提供延时(timeout)事件\n",
    "\n",
    "simpy.Environment.until - 仿真结束的条件（时间或事件）\n",
    "\n",
    "simpy.Environment.run - 仿真启动\n",
    "\n",
    "### example \n",
    "\n",
    "定义一个汽车进程\n",
    "\n",
    "def car(env):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        print('Start parking at %d' % env.now)\n",
    "\n",
    "        parking_duration = 5\n",
    "\n",
    "        yield env.timeout(parking_duration) # 进程延时 5s\n",
    "\n",
    "        print('Start driving at %d' % env.now)\n",
    "\n",
    "        trip_duration = 2\n",
    "\n",
    "        yield env.timeout(trip_duration)   # 延时 2s\n",
    "\n",
    "\n",
    "仿真启动\n",
    "\n",
    "env = simpy.Environment()   # 实例化环境\n",
    "\n",
    "env.process(car(env))   # 添加汽车进程\n",
    "\n",
    "env.run(until=15)   # 设定仿真结束条件, 这里是 15s 后停止\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/31526894\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrival(env,outcomes,R1,n):\n",
    "    lmbda=7\n",
    "    while True: # always holds, so the process will continue indefinitely\n",
    "        inter_arrival=-1/lmbda*np.log(np.random.rand()) #inter_arrival time: exponential distribution # note 10.4\n",
    "        yield env.timeout(inter_arrival)\n",
    "        if env.now < 8 and len(R1.queue) < n: # if the current simulation time is less than 8\n",
    "            # and the length of the queue of recource R1 is less than n\n",
    "            env.process(service(env,outcomes,R1)) # start a new service process\n",
    "        else:\n",
    "            return # end the arrival process\n",
    "\n",
    "def service(env,outcomes,R1):\n",
    "    #send a request to the server\n",
    "    rqt=R1.request()\n",
    "    close=env.timeout(8-env.now)\n",
    "    result =yield rqt|close # | operaror retrun to the first (request to be processed, time out occur)\n",
    "    #customer gets the server\n",
    "    if rqt in result: # equest was successful before the close timeout\n",
    "        #spend x amount of time with the server\n",
    "        service=env.timeout(0.2)\n",
    "        yield service\n",
    "        #release the server so that the server can take care of the next customer\n",
    "        R1.release(rqt)\n",
    "        outcomes[\"served\"]+=1\n",
    "\n",
    "         \n",
    "def system_sim(n):\n",
    "    env=simpy.Environment()\n",
    "    outcomes={\"served\":0}\n",
    "    R1=simpy.Resource(env,capacity=1) \n",
    "    env.process(arrival(env,outcomes,R1,n))\n",
    "    env.run( )\n",
    "    lost=len(R1.queue) # those who have arrived but haven't been served\n",
    "    return outcomes[\"served\"]*4-lost*2\n",
    "\n",
    "def eval_mean(n): # n represents the capacity of the queue\n",
    "  return -np.mean([system_sim(n) for i in range(100)]) # negative, the lower, the less cost and fewer  lost customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.0\n",
      "1 -13.64\n",
      "2 -25.08\n",
      "3 -38.96\n",
      "4 -50.76\n",
      "5 -62.72\n",
      "6 -77.46\n",
      "7 -91.68\n",
      "8 -102.42\n",
      "9 -110.72\n",
      "10 -118.0\n",
      "11 -130.92\n",
      "12 -133.56\n",
      "13 -134.84\n",
      "14 -138.24\n",
      "15 -138.56\n",
      "16 -139.58\n",
      "17 -136.22\n",
      "18 -136.32\n",
      "19 -135.58\n",
      "20 -131.88\n",
      "21 -132.84\n",
      "22 -131.92\n",
      "23 -130.6\n",
      "24 -128.66\n",
      "25 -128.92\n",
      "26 -127.42\n",
      "27 -126.26\n",
      "28 -125.52\n",
      "29 -124.46\n",
      "30 -125.94\n",
      "31 -125.42\n",
      "32 -123.64\n",
      "33 -125.5\n",
      "34 -123.54\n",
      "35 -126.98\n",
      "36 -123.08\n",
      "37 -124.76\n",
      "38 -125.26\n",
      "39 -125.72\n",
      "40 -123.38\n",
      "41 -124.46\n",
      "42 -126.74\n",
      "43 -124.28\n",
      "44 -126.24\n",
      "45 -123.68\n",
      "46 -127.74\n",
      "47 -122.9\n",
      "48 -123.08\n",
      "49 -127.26\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "  print(i, eval_mean(i)) # use grid search to find the best n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPE/random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.66trial/s, best loss: -142.2]\n",
      "{'n': 15.0}\n"
     ]
    }
   ],
   "source": [
    "trials=hyperopt.Trials() # store detailed information about each evaluation during the optimization process\n",
    "best = hyperopt.fmin(fn=eval_mean,\n",
    "    space=hyperopt.hp.quniform('n', 1, 50, 1),\n",
    "    ## random search\n",
    "    #algo=hyperopt.rand.suggest,\n",
    "    ##tpe search \n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also set up an early stopping condition using\n",
    "\n",
    "early_stop_fn=no_progress_loss(iteration_stop_count, percent_increase)\n",
    "\n",
    "stop function that will stop after X iteration if the loss doesn't increase.\n",
    "\n",
    "We can also set timeout= The simulation will stop after certain number of seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:01<00:36,  2.67trial/s, best loss: -129.1]\n",
      "{'n': 24.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt.early_stop import no_progress_loss\n",
    "trials=hyperopt.Trials()\n",
    "best = hyperopt.fmin(fn=eval_mean,\n",
    "    space=hyperopt.hp.quniform('n', 1, 50, 1),\n",
    "    ## random search\n",
    "    #algo=hyperopt.rand.suggest,\n",
    "    ##tpe search \n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=100,\n",
    "    timeout=1, # if the process will be terminated if it exceeds the 1-second timeout, stop\n",
    "    trials=trials,\n",
    "    early_stop_fn=no_progress_loss(iteration_stop_count=10, \n",
    "    percent_increase=0.0)) # if no improvement in the last 10 iterations, stop\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:01<00:21,  4.40trial/s, best loss: -132.1]\n",
      "{'n': 12.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt.early_stop import no_progress_loss\n",
    "trials=hyperopt.Trials()\n",
    "best = hyperopt.fmin(fn=eval_mean,\n",
    "    space=hyperopt.hp.quniform('n', 1, 50, 1),\n",
    "    ## random search\n",
    "    #algo=hyperopt.rand.suggest,\n",
    "    ##tpe search \n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=100,\n",
    "    timeout=1,\n",
    "    trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional help from variance reduction\n",
    "\n",
    "Having a smart search sequence can help us reduce the amount of time to run the simulation. However, at the same, notice that we each decision variable values, we need to run the system multiple times to estimate the population mean.  \n",
    "\n",
    "Due to the randomness in the system, each outcome variable will follow a specific distribution. In many cases, we are interested in knowing the population mean of each distribution. Simulation allows us to directly draw random samples from this distribution. Based on these samples, we can use sample mean to approximate the population mean. We could even construct a confidence interval using these samples to help us construct the confidence interval. \n",
    "\n",
    "The question is how can we ensure that our sample mean is close enough to the population mean? To answer this question, remember that the sample mean varies due to the randomness of sampling. Based on the central limit theorem, when the sample size is large enough (that is, if we run the simulation for many rounds and collect the system output multiple times), the variance of sample mean can be approximated using $\\sigma^2/n$ . This suggests two pathways to ensure that our sample mean has a smaller variation and be close to population mean:\n",
    "\n",
    "- Increase n. \n",
    "    - We simply need to simulate the system for more rounds\n",
    "    - However, this means that we need to run the system for much longer time \n",
    "- Reduce $\\sigma^2$\n",
    "    - The idea is to reduce $\\sigma^2$ in order to reduce $\\sigma^2/n$. The idea is to sample from a different population distribution with the same population mean but a smaller variance.   \n",
    "- Changing n and $\\sigma^2$ at the same time\n",
    "    - We could even find a method that changes both n and $\\sigma^2$ to reduce $\\sigma^2/n$ without changing the mean.  As long as we have a method can reduce $\\sigma^2/n$ without increasing much computation time, we achieve our goal.  \n",
    "\n",
    "If we are planning to sample mnumber of sample outcomes y from a different distribution G instead of n number of sample outcomes x from the original distribution F in order to increase our efficiency. We need to make sure  \n",
    "- Whether (E(x)=E(y)) holds. In other words, both distributions should have the same population mean.\n",
    "- Wether Var(y) < Var(x)\n",
    "- In addition, to achieve this variance reduction, we should not be increasing our computation time much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrating of two methods\n",
    "\n",
    "Let's assume that we are interested in using sample mean to estimate the population mean of a variable X that follows an exponential distribution with $\\lambda$ = 4.  We want the variation of the sample means to be small, as this means when collecting a set of samples, the sample mean as a better chance of being close to the population mean. \n",
    "\n",
    "One method is to generate more samples. \n",
    "\n",
    "Method 1:  let's compare the distribution of sample mean when size=40 versus size=200.\n",
    "\n",
    "It is clear that the variance of the sample mean is much smaller when the sample size increases. \n",
    "\n",
    "One obvious drawback is that we are satisficing the time, as we need to draw more samples.  Can we find an alternative method?\n",
    "\n",
    "Method 2:  Change $\\lambda$ while maintaining μ.\n",
    "\n",
    "Let's assume that an expert told you to try out a normal distribution of mean=0.25 and variance=0.01. You decide to give it a shot and sample 40 from normal distribution instead. let's compare the distribution of sample mean when sampling from our original exponential distribution versus the normal distribution proposed by the other person. In both cases, we use sample size=40 when computing the sample mean. \n",
    "\n",
    "Again, we are able to reduce the variation of the sample mean when sampling from the normal distribution. One can also check the running time to make sure that we are not increasing the running time by too much. Based on the Python demo, we can see that the running time of sampling from a normal distribution is slightly longer but not by much. np.random functions are constructed very efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we are interested in constructing use sample mean to estimate the population mean of an exponential distribution with $\\lambda=4$.\n",
    "\n",
    "Let's use [`np.random.exponential(scale, size=None)`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.exponential.html) to get the job done. \n",
    "Here, `scale` should be equal to $\\frac{1}{\\lambda}$\n",
    "\n",
    "Method 1: Increase sample size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simulation_exp(n):\n",
    "    lm=4\n",
    "    samples=np.random.exponential(1/lm,n)\n",
    "    return np.mean(samples)\n",
    "\n",
    "exp_means_40=[simulation_exp(40) for i in range(1000)]\n",
    "exp_means_200=[simulation_exp(200) for i in range(1000)]\n",
    "\n",
    "plt.hist(np.array(exp_means_40),bins=30,density=True,label=\"s=40\",alpha=0.5)\n",
    "plt.hist(np.array(exp_means_200),bins=30,density=True,label=\"s=200\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"distribution of sample mean size=40 v.s. 200\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: sample from a different distribution \n",
    "\n",
    "Let's assume that another person told you that a normal distribution of mean=0.25 and variance=0.01 can be a good candidate. You decide to give it a shot and sample 40 from normal distribution instead. \n",
    "\n",
    "Let's use [`np.random.normal(loc=0.0, scale=1.0, size=None)`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) to get the job done. \n",
    "\n",
    "Here, `loc` is the population mean, while `scale` is the population standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGxCAYAAABvIsx7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJSElEQVR4nO3deVhUZeM+8HtkYEAElB0UEXcUcAlFUAM1UVTessxy11xzC9FMUl+xVNRKLbdWFUtNK7Pe3MDcAxT3NVdUSnFBBRcWgef3h785X4eZYZ1hOdyf65rrYs76nGfOcvOcTSGEECAiIiKSkWrlXQAiIiIiQ2PAISIiItlhwCEiIiLZYcAhIiIi2WHAISIiItlhwCEiIiLZYcAhIiIi2WHAISIiItlhwCEiIiLZMWjAiYyMhEKh0OhWr149DB06tFjTiYuLQ2RkJB4+fFis8fLPa+/evVAoFPj555+LNZ2CPH36FJGRkdi7d69WvzVr1kChUODatWsGm58xzJgxA3Xr1oVSqUTNmjXLuzgGN3ToUNSrV6+8iyE76u3pxXV/27ZtiIyM1Dm8QqHA+PHjjVaeyrK9DRw4EAqFAr169dLZ/8cff0TLli1hbm4OV1dXhIWF4fHjxyWeX1BQEBQKhc5PRdoudG2nCoVC7/qkT0HrYEWlPlY6Ojri0aNHWv3r1aund32pLPQdj3UdOwuyYsUKrFmzpkRlUJZorGL49ddfYW1tXaxx4uLiMHv2bAwdOrRYB+CSzKu4nj59itmzZwN4viN5Uc+ePREfHw8XFxejlqE0fvvtN8ydOxfTp09HSEgIVCpVeReJKonWrVsjPj4ezZo1k7pt27YNy5cvL5cDTGXY3rZu3YotW7bo3S+tW7cOAwcOxIgRI7B48WJcvHgRH3zwAc6dO4eYmJgSz7d+/fpYt26dVveKvr3Hx8ejTp06xRqnPNfB0rp79y4WLlyIjz/+uLyLYnS69h9FsWLFCtjb2xe7oQQog4DTqlUrY88CGRkZsLCwKJN5FcTBwQEODg7lWobCnDlzBgAwceJEODo6lnNpqDKxtrZGu3btyrsYkoq+vaWlpWH06NH4+OOP8fnnn2v1z83Nxfvvv4/g4GB88803AIBOnTrBysoKAwYMwPbt2xESElKieVtYWFSo36qoKmOZS6N79+5YvHgxxo0bB2dnZ6PMQwiBzMxMWFhYGGX6RVUe+48Sn6LaunUrWrZsCZVKBQ8PD3z66ac6h8vfTJWXl4c5c+agSZMmsLCwQM2aNeHj4yPtACIjI/H+++8DADw8PKSmVXWzlrrpbvPmzWjVqhXMzc2lFhV9p8MyMzMRHh4OZ2dnWFhYIDAwEMePH9cYJigoSKtFBtBsRr127Zq0Q509e7ZUNvU89TWZr1q1Ci1atIC5uTlsbW3Ru3dvnD9/Xms+NWrUwOXLl9GjRw/UqFEDbm5umDx5MrKysnTW7Yvy8vKwcOFCNG3aFCqVCo6Ojhg8eDD++ecfaZh69ephxowZAAAnJ6dCm4OvXr2Kt99+G66urlCpVHByckKXLl1w4sQJaZiNGzciODgYLi4usLCwgKenJ6ZNm4YnT57oXL6///4b3bp1g6WlJVxcXDB//nwAQEJCAjp06ABLS0s0btwY0dHRGuOr6zY2NhbDhg2Dra0tLC0tERoaiqtXrxZaP0IIrFixAi1btoSFhQVq1aqFPn36FGlcdXPyqVOn8Oabb8LGxga2trYIDw9HTk4OLly4gO7du8PKygr16tXDwoULtaaRnp6OKVOmwMPDA2ZmZqhduzbCwsK06mn58uV4+eWX4ejoCEtLS3h7e2PhwoV49uyZxnBBQUHw8vJCYmIiOnbsiOrVq6N+/fqYP38+8vLyClyeN998E82bN9foFhoaCoVCgZ9++knqduzYMSgUCvzvf/8DoN3EPHToUCxfvhwANE6D5F//v//+e3h6eqJ69epo0aIF/vjjjwLLBxS+nwC0tzd1+Ypyambjxo3w9/eHpaUlatSogW7dumntE0pr8uTJcHFxwcSJE3X2T0hIwK1btzBs2DCN7m+++SZq1KiBX3/91aDleZEQAj169ICdnR1u3LghdX/69CmaN28OT09Pad1Ur//Hjx/H66+/Dmtra9jY2GDgwIG4e/dukea3Zs0aNGnSBCqVCp6enli7dq3O4fLvk54+fSptN+r9p6+vLzZs2ACg6OugWlhYGCwtLZGenq7V76233oKTk5PWtqa2e/duBAUFwc7ODhYWFqhbty7eeOMNPH36tEh1oMucOXOQk5NTpNan+/fvY+zYsahduzbMzMxQv359TJ8+Xev4oD41/OWXX8LT0xMqlQrR0dHS9rJ7926MHDkSdnZ2sLa2xuDBg/HkyROkpKSgb9++qFmzJlxcXDBlyhS9dfGiZ8+eYerUqXB2dkb16tXRoUMHHD58WGs4XaeoCjvG1KtXD2fPnsW+fftKdppVlMCuXbuEiYmJ6NChg9i8ebP46aefRJs2bUTdunVF/km6u7uLIUOGSN+joqKEiYmJmDVrlvjzzz/Fjh07xJIlS0RkZKQQQojk5GQxYcIEAUBs3rxZxMfHi/j4eJGWliZNz8XFRdSvX1+sWrVK7NmzRxw+fFjnvPbs2SMACDc3N/Hqq6+K//3vf+KHH34QDRs2FNbW1uLKlSvSsIGBgSIwMFBrWYcMGSLc3d2FEEJkZmaKHTt2CABi+PDhUtkuX74shBBi9erVAoBISkqSxp83b54AIPr16ye2bt0q1q5dK+rXry9sbGzExYsXNeZjZmYmPD09xaeffip27dol/vvf/wqFQiFmz55d6G8yatQoAUCMHz9e7NixQ3z55ZfCwcFBuLm5ibt37wohhDh27JgYPny4ACB27Ngh4uPjRXJyst5pNmnSRDRs2FB8//33Yt++feKXX34RkydPFnv27JGG+fjjj8XixYvF1q1bxd69e8WXX34pPDw8RKdOnbTqUb18n3/+uYiNjRXDhg0TAERERIRo3Lix+O6778TOnTtFr169BABx5MgRaXx13bq5uYl33nlHbN++XXz99dfC0dFRuLm5iQcPHuj8zdRGjhwpTE1NxeTJk8WOHTvE+vXrRdOmTYWTk5NISUkpsG5nzZolAIgmTZqIjz/+WMTGxoqpU6dK9d20aVPxxRdfaCzTL7/8Io3/5MkT0bJlS2Fvby8WLVokdu3aJT7//HNhY2MjOnfuLPLy8qRhJ02aJFauXCl27Nghdu/eLRYvXizs7e3FsGHDNMoUGBgo7OzsRKNGjcSXX34pYmNjxdixYwUAER0dXeDyfPnllwKAuHnzphBCiGfPngkrKythYWEhRo4cKQ23YMECoVQqRXp6uhDi/7Yn9e9/+fJl0adPHwFA2hbi4+NFZmamEEIIAKJevXqibdu2YtOmTWLbtm0iKChIKJVKjW1Pl8L2E0Job29paWka5YiPjxdr164VpqamokePHtJ4c+fOFQqFQrzzzjvijz/+EJs3bxb+/v7C0tJSnD17VhouLy9PPHv2rEif/GJjY4Wpqak4ceKEEOL5vqlnz546f4cX56nm6+sr/P39C6wjfQIDA0Xz5s11ljM3N1ca7t69e6JOnTrCz89PZGdnCyGebzsWFhbi1KlT0nDq9d/d3V28//77YufOnWLRokXC0tJStGrVShpXH/XvlH8f7ObmprWdAhCzZs2Svo8ePVpUr15dLFq0SOzZs0f88ccfYv78+WLp0qVCiMLXwfxOnjwpAIhvvvlGo/uDBw+ESqUS4eHhOsdLSkoS5ubmomvXrmLLli1i7969Yt26dWLQoEEa+56iUtfp3bt3xaRJk4RSqRQXLlyQ+udfXzIyMoSPj4+wtLQUn376qYiJiREzZ84USqVSY90W4nkd1q5dW/j4+Ij169eL3bt3izNnzki/g4eHh5g8ebKIiYkRCxYsECYmJqJfv36idevWYs6cOSI2NlZ88MEHAoD47LPPCl2WIUOGCIVCId5//30RExMjFi1aJGrXri2sra11Ho9fPH4Udow5duyYqF+/vmjVqpX02x47dqzI9VyigOPn5ydcXV1FRkaG1C09PV3Y2toWGnB69eolWrZsWeD0P/nkE62g8OL0TExMNFYGffNSV2jr1q01DiLXrl0TpqamYsSIEVK3ogQcIYS4e/eu1kaoln+H++DBA2FhYaG1At64cUOoVCrRv39/jfkAEJs2bdIYtkePHqJJkyZa83rR+fPnBQAxduxYje6HDh0SAMSHH34odXtxwyrIvXv3BACxZMmSAod7kfqAsG/fPgFAnDx5UuqnXr4XD/zPnj0TDg4OAoDGSpuamipMTEw0djbquu3du7fGPP/66y8BQMyZM0djXi/+ZvHx8To31uTkZGFhYSGmTp1a4HKp6yz/+C1btpSCeP5lev3116VuUVFRolq1aiIxMVFj/J9//lkAENu2bdM539zcXPHs2TOxdu1aYWJiIu7fvy/1CwwMFADEoUOHNMZp1qyZ6NatW4HLc/nyZQFArF27VgghxMGDBwUAMXXqVOHh4SEN17VrVxEQECB917WDGjdunNY2rwZAODk5SQFJCCFSUlJEtWrVRFRUVIFlLMp+Qtc/FC+6ffu2qF+/vmjevLl0ELpx44ZQKpViwoQJGsM+evRIODs7i759+2pNvyif/NOqV6+eiIiIkLrpCjhz584VAMStW7e0yh4cHCwaN25c4PLro143dH2GDx+uMezBgweFUqkUYWFhYtWqVQKA+PbbbzWGUa//kyZN0ui+bt06AUD88MMPesuSm5srXF1d9e6DCws4Xl5e4rXXXitweQtaB3Vp3bq1xnothBArVqwQAMTp06d1jqPeVtWBtbRe3A/fu3dP2NjYiDfeeEPqn399UYfh/MeHBQsWCAAiJiZG6gZA2NjYaOwvhPi/9Tn/uv/aa68JAGLRokUa3Vu2bClat25d4HKojz361o2CAk5RjzHNmzfXeWwuimKfonry5AkSExPx+uuvw9zcXOpuZWWF0NDQQsdv27YtTp48ibFjx2Lnzp06mwoL4+Pjg8aNGxd5+P79+2vc3eXu7o6AgADs2bOn2PMujvj4eGRkZGidNnNzc0Pnzp3x559/anRXKBRadejj44Pr168XOB/1cuSfT9u2beHp6ak1n6KwtbVFgwYN8Mknn2DRokU4fvy4zlMfV69eRf/+/eHs7AwTExOYmpoiMDAQALROwykUCvTo0UP6rlQq0bBhQ7i4uGhcP2VrawtHR0edyz1gwACN7wEBAXB3dy/wt/zjjz+gUCgwcOBA5OTkSB9nZ2e0aNGiyFf157+rwdPTEwqFQuM6CfUyvVj2P/74A15eXmjZsqXG/Lt166bVZHv8+HH85z//gZ2dnVSfgwcPRm5uLi5evKgxf2dnZ7Rt21ajW1HWlwYNGqBevXrYtWsXACA2Nhbe3t4YOHAgkpKScOXKFWRlZeHgwYN45ZVXilQ3+qivKVFzcnLS+9u+qLT7iSdPnqBnz57IzMzE9u3bpZsVdu7ciZycHAwePFjjtzA3N0dgYKDGbxEaGorExMQifV40bdo0mJqa4r///W+Rypr/ztPCuhdFgwYNdJZz5syZGsO1b98ec+fOxZIlS/Duu+9i4MCBGD58uM5p5t/2+vbtC6VSWeC2d+HCBdy8eVPvPrgwbdu2xfbt2zFt2jTs3bsXGRkZhY5TmGHDhiEuLg4XLlyQuq1evRpt2rSBl5eXznFatmwJMzMzjBo1CtHR0UU6tV1UdnZ2+OCDD/DLL7/g0KFDOofZvXs3LC0t0adPH43u6n1+/n18586dUatWLZ3T0rUfA55ftJ+/e1GPPfrWjYIU9RhTGsUOOA8ePEBeXp7OC6KKcpFUREQEPv30UyQkJCAkJAR2dnbo0qULjhw5UuQyFPeuCX1lTU1NLdZ0iks9fV3ldXV11Zp/9erVNUIj8Pyuh8zMTIPOpygUCgX+/PNPdOvWDQsXLkTr1q3h4OCAiRMnSrc1Pn78GB07dsShQ4cwZ84c7N27F4mJidi8eTMAaO2MdC2fmZkZbG1tteZvZmamc7lL8lvevn0bQgg4OTnB1NRU45OQkIB79+4VXiGAVjnNzMz0LtOLZb99+zZOnTqlNW8rKysIIaT537hxAx07dsS///6Lzz//HAcOHEBiYqJ0jUH++rSzs9Mqo0qlKtJBoEuXLtJOcdeuXejatSu8vb3h5OSEXbt24a+//kJGRkapA05Jy1ia/UROTg769OmDixcvYtu2bXBzc5P63b59GwDQpk0brd9j48aNGuuCra0tWrZsWaSP2uHDh7FixQosXLgQmZmZePjwIR4+fIi8vDzk5OTg4cOH0jUT6rrRte7ev39f53ZRVObm5vD19dX6uLu7aw07YMAAmJmZISsrS7r+UZf8255SqYSdnV2B2566X0mPF1988QU++OADbNmyBZ06dYKtrS1ee+01XLp0qdBx9RkwYABUKpV06/G5c+eQmJiodS3Uixo0aIBdu3bB0dER48aNQ4MGDdCgQQOdF4+XRFhYGFxdXTF16lSd/VNTU+Hs7KwVeh0dHaFUKrV+g4KOkbr2Y/q6F/XYo2/dKEhRjjGlVey7qGrVqgWFQoGUlBStfrq6ac1QqUR4eDjCw8Px8OFD7Nq1Cx9++CG6deuG5ORkVK9evdBpFPc/G31lffEHMDc3R1pamtZwRT346aKe/q1bt7T63bx5E/b29iWetr755L/FsjTzcXd3x3fffQcAuHjxIjZt2oTIyEhkZ2fjyy+/xO7du3Hz5k3s3btXarUBUOznFxWHvt+yYcOGesext7eHQqHAgQMHdN4ma+xbZ+3t7WFhYYFVq1bp7Q8AW7ZswZMnT7B582aNA9GLF3UbSpcuXfDdd9/h8OHDOHTokHTxeefOnREbG4vr16+jRo0a5XZXS2n2E6NGjcKff/6Jbdu2oUWLFhr91HX9888/6zzYvyg6OrrAg96LhBAAnh8shRDo3bu31jDJycmoVasWFi9ejLCwMHh7ewMATp8+rXHrbE5ODv7++2/069evSPMujdzcXAwYMAC1atWCSqXC8OHD8ddff0kHvRelpKSgdu3aGuVMTU0t8ECm7lfS44WlpSVmz56N2bNn4/bt21JrTmhoKP7++++iLKKWWrVq4dVXX8XatWsxZ84crF69Gubm5oXWd8eOHdGxY0fk5ubiyJEjWLp0KcLCwuDk5IS33367RGVRs7CwQGRkJEaNGoWtW7dq9bezs8OhQ4cghNA4/t25cwc5OTla+/jStP4Vx4u/r651ozCFHWNKq9gtOJaWlmjbti02b96ske4ePXok3W1RVDVr1kSfPn0wbtw43L9/X7ryXX3AMURzJABs2LBB2gEBwPXr1xEXF6dx11S9evVw8eJFjSvSU1NTERcXpzGt4pTN398fFhYW+OGHHzS6//PPP9i9eze6dOlSksXR0rlzZwDQmk9iYiLOnz9vkPk0btwYM2bMgLe3N44dOwbg/zai/AHhq6++KvX89Mn/bI+4uDhcv35d5x1war169YIQAv/++6/O/2rVBxpj6dWrF65cuQI7Ozud81ffFaCrPoUQ0i3EhtSlSxcoFArMnDkT1apVw8svvwwAeOWVV7Bnzx7Exsbi5ZdfhqmpaYHTMfS2qou+/YQuM2bMwOrVq/Htt9/qbH3q1q0blEolrly5ovO38PX1lYYtySmq7t27Y8+ePVofJycntGvXDnv27JFOM/j5+cHFxUXrIWY///wzHj9+jNdff710FVcEs2bNwoEDB7Bu3Tps3LgRJ0+e1NuKk3/b27RpE3Jycgrc9po0aQIXFxe9++DicHJywtChQ9GvXz9cuHBBunupJOvgsGHDcPPmTWzbtg0//PADevfuXeRnrpmYmMDPz09qWVXvD0vrnXfeke5CzX+qpkuXLnj8+DG2bNmi0V19N5qhjiXFpf7t9a0bxaHrGAMUvVValxI9B+fjjz9G9+7d0bVrV0yePBm5ublYsGABLC0tcf/+/QLHDQ0NhZeXF3x9feHg4IDr169jyZIlcHd3R6NGjQBAOuB8/vnnGDJkCExNTdGkSRONc/nFcefOHfTu3RsjR45EWloaZs2aBXNzc0REREjDDBo0CF999RUGDhyIkSNHIjU1FQsXLtR6QJeVlRXc3d3x22+/oUuXLrC1tYW9vb3OW9dq1qyJmTNn4sMPP8TgwYPRr18/pKamYvbs2TA3N8esWbNKtDz5NWnSBKNGjcLSpUtRrVo1hISE4Nq1a5g5cybc3NwwadKkYk/z1KlTGD9+PN588000atQIZmZm2L17N06dOoVp06YBeH79S61atTBmzBjMmjULpqamWLduHU6ePGmQ5dLlyJEjGDFiBN58800kJydj+vTpqF27NsaOHat3nPbt22PUqFEYNmwYjhw5gpdffhmWlpa4desWDh48CG9vb7z77rtGK3NYWBh++eUXvPzyy5g0aRJ8fHyQl5eHGzduICYmBpMnT4afnx+6du0KMzMz9OvXD1OnTkVmZiZWrlyJBw8eGLxMjo6O8PLyQkxMDDp16iS1iLzyyiu4f/8+7t+/j0WLFhU6HfW2umDBAoSEhMDExAQ+Pj46WwCKoyj7ifx++uknzJ07F3369EHjxo2RkJAg9VOpVGjVqhXq1auHjz76CNOnT8fVq1fRvXt31KpVC7dv38bhw4elFgPg+X+nhTWz5+fs7Kzz1Iu5uTns7Ow0woCJiQkWLlyIQYMGYfTo0ejXrx8uXbqEqVOnomvXrujevbvGNBQKhdZ1QvpkZGRoLP+L1K1ysbGxiIqKwsyZM6UDZFRUFKZMmYKgoCCtVqjNmzdDqVSia9euOHv2LGbOnIkWLVqgb9++estRrVo1fPzxxxgxYoS0D3748CEiIyOLdIrKz88PvXr1go+PD2rVqoXz58/j+++/h7+/v7TOlmQdDA4ORp06dTB27FikpKRotdSpW4QvX74MAFKLdc+ePVG3bl1kZmZKLbIvBun84xWHiYkJ5s2bJ9W7j4+P1G/w4MFYvnw5hgwZgmvXrsHb2xsHDx7EvHnz0KNHj1KfSi4pT09PDBw4EEuWLIGpqSleeeUVnDlzBp9++mmhD90tyjEGeP77/vjjj9i4cSPq168Pc3Pzov9TWqJLk4UQv//+u/Dx8RFmZmaibt26Yv78+dKV4S/Kf2fTZ599JgICAoS9vb007vDhw8W1a9c0xouIiBCurq6iWrVqGlde67obQd+81Fdtf//992LixInCwcFBqFQq0bFjR41bkNWio6OFp6enMDc3F82aNRMbN27Uecvxrl27RKtWrYRKpdK4UlzfXR3ffvutVFc2Njbi1Vdf1bo1dMiQIcLS0lKrTLrqVJfc3FyxYMEC0bhxY2Fqairs7e3FwIEDtW4DL+pdVLdv3xZDhw4VTZs2FZaWlqJGjRrCx8dHLF68WOTk5EjDxcXFCX9/f1G9enXh4OAgRowYIY4dOyYAiNWrVxe6fOpbWvPL/zur6zYmJkYMGjRI1KxZU7pD7dKlSxrj6vrNhBBi1apVws/PT1haWgoLCwvRoEEDMXjwYJ3rwov01Vlxlunx48dixowZokmTJtJ64O3tLSZNmqRxm/r//vc/0aJFC2Fubi5q164t3n//fbF9+3atu5f01Zu+Zddl0qRJAoCYO3euRvdGjRoJABq3Cguh+y6qrKwsMWLECOHg4CAUCoXG+g9AjBs3Tmu++bdTXYqyn8i/val/J12f/HWyZcsW0alTJ2FtbS1UKpVwd3cXffr0Ebt27Sq40kqooP3W+vXrpf2Ds7OzmDhxonj06JHGMI8ePRIAxNtvv13ovAq6iwqAePbsmbh586ZwdHQUnTt31rh1PC8vT4SGhoqaNWtq1evRo0dFaGioqFGjhrCyshL9+vUTt2/fLtLyf/vtt6JRo0bCzMxMNG7cWKxatUrnuop8d1FNmzZN+Pr6ilq1agmVSiXq168vJk2aJO7duycNU9A6WJAPP/xQ4P8/euLFOhDi+e+V/07M3r17C3d3d6FSqYSdnZ0IDAwUv//+e4Hj6VPQfjggIEAA0FpfUlNTxZgxY4SLi4tQKpXC3d1dREREaN0Sr2+7U28v+e/mLO7+Lb+srCwxefJk4ejoKMzNzUW7du1EfHy83uOxev9R1GPMtWvXRHBwsLCystK5LRdEIcQL7YZEFdSaNWswbNgwJCYmapxGIKoKtm3bhl69euHkyZNGP6WaX2RkJGbPno27d+8a7LpBorLAt4kTEVVwe/bswdtvv13m4YaoMjP6u6iIiKh0Pvnkk/IuAlGlw1NUREREJDs8RUVERESyw4BDREREssOAQ0RERLJT4S4yzsvLw82bN2FlZVVmj5smIiKi0hFC4NGjR3B1dUW1auXfflLhAs7Nmzc1Xo5HRERElUdycrLWexHLQ4ULOOrXMSQnJxf6qGciIiKqGNLT0+Hm5lbi1yoZWoULOOrTUtbW1gw4RERElUxFubyk/E+SERERERkYAw4RERHJDgMOERERyU6FuwaHiKgwQgjk5OQgNze3vItCVKWYmJhAqVRWmOtsCsKAQ0SVSnZ2Nm7duoWnT5+Wd1GIqqTq1avDxcUFZmZm5V2UAjHgEFGlkZeXh6SkJJiYmMDV1RVmZmaV4j9JIjkQQiA7Oxt3795FUlISGjVqVCEe6KcPAw4RVRrZ2dnIy8uDm5sbqlevXt7FIapyLCwsYGpqiuvXryM7Oxvm5ublXSS9Km70IiLSoyL/10gkd5Vl+6scpSQiIiIqBgYcIiIikh0GHCIiIpIdBhwiIiKSHd5FRUSysDj2YpnOb1LXxmU6v+JITU2Fp6cnDh8+jHr16gEAZsyYgU8++QRvvPEG1q9fX25l69OnDwICAhAeHl5uZaCqgS04REQyExUVhdDQUCncAMDUqVOxaNEibNiwAZcvXy6TMigUCoSFhWl0/+9//4u5c+ciPT3d6GWgqo0tOERysidKd/dOEWVbDio3GRkZ+O6777Bt2zaN7tbW1njnnXcwceJEnD59Gg0bNjRaGRITE/H111/Dx8dHq5+Pjw/q1auHdevW4d133zVaGYjYgkNEVEaEEFi4cCHq168PCwsLtGjRAj///DMA4O7du3B2dsa8efOk4Q8dOgQzMzPExMQAAIKCgjB+/HiMHz8eNWvWhJ2dHWbMmAEhhDTO9u3boVQq4e/vrzX/nJwcVK9eHWfOnDHaMj5+/BgDBgzAN998g1q1aukc5j//+Q82bNhgtDIQAQw4RERlZsaMGVi9ejVWrlyJs2fPYtKkSRg4cCD27dsHBwcHrFq1CpGRkThy5AgeP36MgQMHYuzYsQgODpamER0dDaVSiUOHDuGLL77A4sWL8e2330r99+/fD19fX73zf/z4cYEBZ968eahRo0aBnwMHDugdf9y4cejZsydeeeUVvcO0bdsWhw8fRlZWVkHVRVQqPEVFRFQGnjx5gkWLFmH37t1S60r9+vVx8OBBfPXVVwgMDESPHj0wcuRIDBgwAG3atIG5uTnmz5+vMR03NzcsXrwYCoUCTZo0wenTp7F48WKMHDkSAHDt2jW4urpqzf/o0aP48ssv0bNnzwIDzpgxY9C3b98Cl6V27do6u//44484duwYEhMTCx0/KysLKSkpcHd3L3BYopJiwCEiKgPnzp1DZmYmunbtqtE9OzsbrVq1kr5/+umn8PLywqZNm3DkyBGtd/20a9dO4wWj/v7++Oyzz5CbmwsTExNkZGRojZOXl4fRo0dj/Pjx8PPzw4ABA5Cdna3zbdC2trawtbUt9vIlJyfjvffeQ0xMTKHvJ7KwsAAAvhGejIoBh4ioDOTl5QEAtm7dqtUColKppL+vXr2KmzdvIi8vD9evX9d5oW5B7O3t8eDBA41uS5cuxd27d/HRRx/hxo0byMnJwYULF+Dt7a01/rx58zSuA9Jl+/bt6Nixo0a3o0eP4s6dO3jppZekbrm5udi/fz+WLVuGrKwsmJiYAADu378PAHBwcCjWshEVBwMOEVEZaNasGVQqFW7cuIHAwECdw2RnZ2PAgAF466230LRpUwwfPhynT5+Gk5OTNExCQoLGOAkJCWjUqJEUHlq1aoUffvhB6v/vv/9i5syZ2LBhAywtLdGoUSOoVCqcOXNGZ8Ap6SmqLl264PTp0xrdhg0bhqZNm+KDDz6QygcAZ86cQZ06dWBvb1/gfIhKgwGHiKgMWFlZYcqUKZg0aRLy8vLQoUMHpKenIy4uDjVq1MCQIUMwffp0pKWl4YsvvkCNGjWwfft2DB8+HH/88Yc0neTkZISHh2P06NE4duwYli5dis8++0zq361bN0RERODBgweoVasWJk6ciJCQEPTs2RMAoFQq4enpqfc6nJKeorKysoKXl5dGN0tLS9jZ2Wl1P3DggMaF00TGwIBDRLJQkZ8srPbxxx/D0dERUVFRuHr1KmrWrInWrVvjww8/xN69e7FkyRLs2bMH1tbWAIDvv/8ePj4+WLlypfTMmMGDByMjIwNt27aFiYkJJkyYgFGjRknz8Pb2hq+vLzZt2oTatWtj9+7dOH/+vEY5vL29jXqreEEyMzPx66+/YufOneUyf6o6FOLFByhUAOnp6bCxsUFaWpq0kRNREcn8QX+ZmZlISkqCh4dHoReyylFQUBBatmyJJUuWFDjctm3bMGXKFJw5cwbVqlWsp4EsX74cv/32m/RsH6p89G2HFe34zRYcIiKZ6dGjBy5duoR///0Xbm5u5V0cDaampli6dGl5F4OqAAYcIiIZeu+998q7CDq9eDqNyJgYcIiIKom9e/eWdxGIKo2KdXKWiIiIyAAYcIiIiEh2GHCIiIhIdhhwiIiISHYYcIiIiEh2GHCIiIhIdnibOFFVoOsJxzJ5ujERkS5swSEiIiLZYcAhIiIi2WHAISIiKqLk5GQEBQWhWbNm8PHxwU8//VTeRSI9eA0OUUVRnOtk9L01nIiMSqlUYsmSJWjZsiXu3LmD1q1bo0ePHrC0tCzvolE+bMEhIqrCUlNT4ejoiGvXrpV3USoFFxcXtGzZEgDg6OgIW1tb3L9/v3wLZUB9+vTBokWLyrsYBsGAQ0RUhUVFRSE0NBT16tUr83mvWLECHh4eMDc3x0svvYQDBw4UOHxUVBTatGkDKysrODo64rXXXsOFCxc0homMjIRCodD4ODs7G6X8R44cQV5eHtzc3Aw+7eLWTVHG2b9/P0JDQ+Hq6gqFQoEtW7ZoTeO///0v5s6di/T0dEMtSrnhKSoikoeyPm1XQW6zz87OhpmZWYnGzcjIwHfffYdt27YZuFSF27hxI8LCwrBixQq0b98eX331FUJCQnDu3DnUrVtX5zj79u3DuHHj0KZNG+Tk5GD69OkIDg7GuXPnNE4RNW/eHLt27ZK+m5iYGLz8qampGDx4ML799luDT7skdVOUcZ48eYIWLVpg2LBheOONN3ROx8fHB/Xq1cO6devw7rvvGnzZyhJbcIiIykhQUBAmTpyIqVOnwtbWFs7OzoiMjJT6Z2VlYeLEiXB0dIS5uTk6dOiAxMRErWmMHz8e4eHhsLe3R9euXaXuEyZMQFhYGGrVqgUnJyd8/fXXePLkCYYNGwYrKys0aNAA27dvl6a1fft2KJVK+Pv7S90OHjwIU1NTZGVlSd2SkpKgUChw/fp1g9XFokWLMHz4cIwYMQKenp5YsmQJ3NzcsHLlSr3j7NixA0OHDkXz5s3RokULrF69Gjdu3MDRo0c1hlMqlXB2dpY+Dg4OxSpbQkICunTpAnt7e63WoIcPHyIrKwu9e/dGREQEAgICSrT8BSlJ3RRlnJCQEMyZMwevv/56gfP/z3/+gw0bNhhsecoLAw4RURmKjo6GpaUlDh06hIULF+Kjjz5CbGwsAGDq1Kn45ZdfEB0djWPHjqFhw4bo1q2b1jUe0dHRUCqV+Ouvv/DVV19pdLe3t8fhw4cxYcIEvPvuu3jzzTcREBCAY8eOoVu3bhg0aBCePn0K4PkpC19fX41pnzhxAp6enlCpVBrdatasCXd3d63lmTdvHmrUqFHgJ/+pkuzsbBw9ehTBwcEa3YODgxEXF1fkukxLSwMA2NraanS/dOkSXF1d4eHhgbfffhtXr14t8jRPnjyJoKAgtGjRAvv378eOHTtga2uLTp06YePGjbCxscHQoUPRuXNnDBo0SO90SlIvQMnqxlD1qda2bVscPnxYI+RWRjxFRURUhnx8fDBr1iwAQKNGjbBs2TL8+eefCAgIwMqVK7FmzRqEhIQAAL755hvExsbiu+++w/vvvy9No2HDhli4cKHWtFu0aIEZM2YAACIiIjB//nzY29tj5MiRAJ5fX7Fy5UqcOnUK7dq1w7Vr1+Dq6qoxjZMnT6JVq1Ya3U6cOIEWLVroXJ4xY8agb9++BS5z7dq1Nb7fu3cPubm5cHJy0uju5OSElJSUAqelJoRAeHg4OnToAC8vL6m7n58f1q5di8aNG+P27duYM2cOAgICcPbsWdjZ2RU63YkTJ+LVV1+VLrRt1qwZ+vXrh0OHDqFv3744ePAgNm7cCB8fH+kalu+//x7e3t4a0ylJvQAlqxtD1Gf+cmVlZSElJUVnqK0sGHCIiMqQj4+PxncXFxfcuXMHV65cwbNnz9C+fXupn6mpKdq2bYvz589rjJO/1UXXtE1MTGBnZ6dx4FUfAO/cuQPg+TU45ubmGtM4ceIE+vfvr9Ht+PHjegOOra2tVgtKUSkUCo3vQgitbvqMHz8ep06dwsGDBzW6q8MhAHh7e8Pf3x8NGjRAdHQ0wsPDC5zm7du3cfDgQezevVuju6WlpVSuDh06IC8vr9DylaZegJLVTWnq80UWFhYAILX0VVY8RUVEVIZMTU01visUCuTl5UEIIX1/ka6DlL5nruia9ovd1NNRH6Dt7e3x4MEDqX9ubi7Onj2r1YJz7Ngx6dbo/EpyKsbe3h4mJiZarQt37tzRaoXQZcKECfj999+xZ88e1KlTp8BhLS0t4e3tjUuXLhU63aNHjyIvL08rzB09elRvqNSnpKeoSlI3pa3P/NSnRIt77VJFwxYcIqIKoGHDhjAzM8PBgwelFpRnz57hyJEjCAsLM8o8W7VqhR9++EH6fuHCBWRkZGictoqPj8e///5r0FNUZmZmeOmllxAbG4vevXtL3WNjY/Hqq6/qnY4QAhMmTMCvv/6KvXv3wsPDo8D5As8v3D5//jw6duxY6LDq4JeRkYGaNWsCAE6fPo39+/fjo48+KnT8F5X0FFVJ6qak9anPmTNnUKdOHdjb2xd73IqEAYeIqAKwtLTEu+++i/fffx+2traoW7cuFi5ciKdPn2L48OFGmWe3bt0QERGBBw8eoFatWjhx4gQAYOnSpZg4cSIuX76MiRMnAoDeC05LeiomPDwcgwYNgq+vL/z9/fH111/jxo0bGDNmjDTMsmXL8Ouvv+LPP/8EAIwbNw7r16/Hb7/9BisrK6nFwsbGRjqtMmXKFISGhqJu3bq4c+cO5syZg/T0dAwZMqTQMvn5+cHCwgJTp07F9OnTceXKFUyYMAFjxowp9t1SpTlFVZK6Kco4jx8/xuXLl6XvSUlJOHHihLS+qR04cEDrguXKqFgBJyoqCps3b8bff/8NCwsLBAQEYMGCBWjSpIk0zNChQxEdHa0xnp+fHxISEgxTYiIimZo/fz7y8vIwaNAgPHr0CL6+vti5cydq1apllPl5e3vD19cXmzZtwujRo3HixAl07doVSUlJ8PLyQrNmzTB//ny88847WL58ucbt5KX11ltvITU1FR999BFu3boFLy8vbNu2TeOi1nv37uHKlSvSd/Utz0FBQRrTWr16NYYOHQoA+Oeff9CvXz/cu3cPDg4OaNeuHRISEjSmu2bNGgwbNkw6Lajm4OCATZs2YfLkyfDx8YGbmxvGjBmDKVOmGGy5i6IkdVOUcY4cOYJOnTpJ39XXJA0ZMgRr1qwBAGRmZuLXX3/Fzp07jbyUxqcQ+X/hAnTv3h1vv/22xkOWTp8+rfGQpaFDh+L27dtYvXq1NJ6ZmVmRk2x6ejpsbGyQlpYGa2vrYi4OUSVW1u+iqiAPqiuOzMxMJCUlSU9r1VBFH/RXWtu2bcOUKVNw5swZhISEoHXr1oiKkve7ziIjI7F3717s3bu3vItS4Sxfvhy//fYbYmJi9A6jbzusaMfvYrXg7NixQ+P76tWr4ejoiKNHj+Lll1+WuqtUKqM9GpuISCeZBI6y1qNHD1y6dAn//vsvTp48KbWEyNnOnTvx+eefl3cxKiRTU1MsXbq0vIthEKW6BkffQ5b27t0LR0dH1KxZE4GBgZg7dy4cHR11TiMrK0vj3K4c3n9BRFSZvPfee0hJScHt27e1bmOXo/j4+PIuQoU1atSo8i6CwZT4NnF9D1kKCQnBunXrsHv3bnz22WdITExE586d9V6gFhUVBRsbG+ljjJeWERFRwZydnSGEQPPmzcu7KEQGUeIWHH0PWXrrrbekv728vODr6wt3d3ds3bpV5/svIiIiNB6+lJ6ezpBDREREpVKigKN+yNL+/fsLfciSi4sL3N3d9T5kSaVSabzzhIiIiKi0ihVwSvKQpdTUVCQnJ8PFxaXEhSQiIiIqjmJdgzNu3Dj88MMPWL9+vfSQpZSUFGRkZAB4/hChKVOmID4+HteuXcPevXsRGhoKe3t7jacrEhERERlTsVpwCnvIkomJCU6fPo21a9fi4cOHcHFxkV4xb2VlZbBCExERERWk2KeoCmJhYSGLpx8SERFR5ca3iRNRpaN+KSIRlb3Ksv3xZZtEVGmYmZmhWrVquHnzJhwcHGBmZgaFQlHexSKqEoQQyM7Oxt27d1GtWjWYmZmVd5EKxIBDRJVGtWrV4OHhgVu3buHmzZvlXRyiKql69eqoW7cuqlWr2CeBGHCIqFIxMzND3bp1kZOTg9zc3PIuDlGVYmJiAqVSWSlaThlwiKjSUSgUMDU1hampaXkXhYgqqIrdvkRERERUAgw4REREJDsMOERERCQ7DDhEREQkOww4REREJDsMOERERCQ7DDhEREQkOww4REREJDsMOERERCQ7DDhEREQkOww4REREJDsMOERERCQ7DDhEREQkOww4REREJDsMOERERCQ7DDhEREQkOww4REREJDsMOERERCQ7yvIuABEVYE9UeZeAiKhSYgsOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyU6xAk5UVBTatGkDKysrODo64rXXXsOFCxc0hhFCIDIyEq6urrCwsEBQUBDOnj1r0EITERERFaRYAWffvn0YN24cEhISEBsbi5ycHAQHB+PJkyfSMAsXLsSiRYuwbNkyJCYmwtnZGV27dsWjR48MXngiIiIiXRRCCFHSke/evQtHR0fs27cPL7/8MoQQcHV1RVhYGD744AMAQFZWFpycnLBgwQKMHj260Gmmp6fDxsYGaWlpsLa2LmnRiCqfPVFlO79OEWU7PyKStYp2/C7VNThpaWkAAFtbWwBAUlISUlJSEBwcLA2jUqkQGBiIuLg4ndPIyspCenq6xoeIiIioNEoccIQQCA8PR4cOHeDl5QUASElJAQA4OTlpDOvk5CT1yy8qKgo2NjbSx83NraRFIiIiIgJQioAzfvx4nDp1Chs2bNDqp1AoNL4LIbS6qUVERCAtLU36JCcnl7RIRERERAAAZUlGmjBhAn7//Xfs378fderUkbo7OzsDeN6S4+LiInW/c+eOVquOmkqlgkqlKkkxiIiIiHQqVguOEALjx4/H5s2bsXv3bnh4eGj09/DwgLOzM2JjY6Vu2dnZ2LdvHwICAgxTYiIiIqJCFKsFZ9y4cVi/fj1+++03WFlZSdfV2NjYwMLCAgqFAmFhYZg3bx4aNWqERo0aYd68eahevTr69+9vlAUgIiIiyq9YAWflypUAgKCgII3uq1evxtChQwEAU6dORUZGBsaOHYsHDx7Az88PMTExsLKyMkiBiYiIiApTqufgGENFu4+eqMzwOThEVIlVtOM330VFREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLJToicZE1EplPXdUvroKwfvriIiGWALDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOAw4RERHJDgMOERERyQ4DDhEREckOX9VAREXDVzsQUSXCFhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdZXkXgIgqmD1R5V0CIqJSYwsOERERyU6xA87+/fsRGhoKV1dXKBQKbNmyRaP/0KFDoVAoND7t2rUzVHmJiIiIClXsgPPkyRO0aNECy5Yt0ztM9+7dcevWLemzbdu2UhWSiIiIqDiKfQ1OSEgIQkJCChxGpVLB2dm5xIUiIiIiKg2jXIOzd+9eODo6onHjxhg5ciTu3Lmjd9isrCykp6drfIiIiIhKw+ABJyQkBOvWrcPu3bvx2WefITExEZ07d0ZWVpbO4aOiomBjYyN93NzcDF0kIiIiqmIMfpv4W2+9Jf3t5eUFX19fuLu7Y+vWrXj99de1ho+IiEB4eLj0PT09nSGHiIiISsXoz8FxcXGBu7s7Ll26pLO/SqWCSqUydjGIiIioCjH6c3BSU1ORnJwMFxcXY8+KiIiICEAJWnAeP36My5cvS9+TkpJw4sQJ2NrawtbWFpGRkXjjjTfg4uKCa9eu4cMPP4S9vT169+5t0IITERER6VPsgHPkyBF06tRJ+q6+fmbIkCFYuXIlTp8+jbVr1+Lhw4dwcXFBp06dsHHjRlhZWRmu1EREREQFKHbACQoKghBCb/+dO3eWqkBEREREpcV3UREREZHsMOAQERGR7DDgEBERkeww4BAREZHsMOAQERGR7DDgEBERkeww4BAREZHsMOAQERGR7DDgEBERkeww4BAREZHsMOAQERGR7DDgEBERkeww4BAREZHsMOAQERGR7DDgEBERkeww4BAREZHsMOAQERGR7CjLuwBEsrUnqrxLUDb0LWeniLItBxHRC9iCQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDt4kTFVVVeTu4oeiqL75hnIjKCFtwiIiISHYYcIiIiEh2GHCIiIhIdhhwiIiISHYYcIiIiEh2GHCIiIhIdhhwiIiISHYYcIiIiEh2GHCIiIhIdhhwiIiISHYYcIiIiEh2GHCIiIhIdhhwiIiISHYYcIiIiEh2GHCIiIhIdhhwiIiISHYYcIiIiEh2GHCIiIhIdhhwiIiISHYYcIiIiEh2ih1w9u/fj9DQULi6ukKhUGDLli0a/YUQiIyMhKurKywsLBAUFISzZ88aqrxEREREhSp2wHny5AlatGiBZcuW6ey/cOFCLFq0CMuWLUNiYiKcnZ3RtWtXPHr0qNSFJSIiIioKZXFHCAkJQUhIiM5+QggsWbIE06dPx+uvvw4AiI6OhpOTE9avX4/Ro0eXrrRERERERVDsgFOQpKQkpKSkIDg4WOqmUqkQGBiIuLg4nQEnKysLWVlZ0vf09HRDFomoTMRfTS3xuP717QxYEiIiAgx8kXFKSgoAwMnJSaO7k5OT1C+/qKgo2NjYSB83NzdDFomIiIiqIKPcRaVQKDS+CyG0uqlFREQgLS1N+iQnJxujSERERFSFGPQUlbOzM4DnLTkuLi5S9zt37mi16qipVCqoVCpDFoOIiIiqOIO24Hh4eMDZ2RmxsbFSt+zsbOzbtw8BAQGGnBURERGRXsVuwXn8+DEuX74sfU9KSsKJEydga2uLunXrIiwsDPPmzUOjRo3QqFEjzJs3D9WrV0f//v0NWnAi4sXNRET6FDvgHDlyBJ06dZK+h4eHAwCGDBmCNWvWYOrUqcjIyMDYsWPx4MED+Pn5ISYmBlZWVoYrNREREVEBih1wgoKCIITQ21+hUCAyMhKRkZGlKRcRERFRiRn0ImMiKr7SnGYiIiLd+LJNIiIikh0GHCIiIpIdnqIiqqJ4BxYRyRlbcIiIiEh2GHCIiIhIdhhwiIiISHYYcIiIiEh2GHCIiIhIdhhwiIiISHZ4mzjR/7c49mKB/dvd4BOHiYgqC7bgEBERkeww4BAREZHs8BQVVTiFnSoqyKSujQ1YEiIiqqzYgkNERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDJxmTrJTmKchERCQfbMEhIiIi2WELDhEVW/zV1BKNl5Bzke8LI6IywRYcIiIikh0GHCIiIpIdnqIiojLT7sbXwB477R6dIsq+MEQka2zBISIiItlhwCEiIiLZYcAhIiIi2WHAISIiItnhRcZEVKZ0PUMnIafoT6Dmc3SIqCjYgkNERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssMH/ZFRLI4t+oPbiIiIDI0tOERERCQ7DDhEREQkOww4REREJDsMOERERCQ7DDhEREQkOww4REREJDsMOERERCQ7DDhEREQkOww4REREJDsGf5JxZGQkZs+erdHNyckJKSkphp4VEVVBpXlK9qSujQ1YEiKqyIzyqobmzZtj165d0ncTExNjzIaIiIhIJ6MEHKVSCWdn5yINm5WVhaysLOl7enq6MYpEREREVYhRAs6lS5fg6uoKlUoFPz8/zJs3D/Xr19c5bFRUlNYpLaLy1u7G1+VdBCIiKgWDX2Ts5+eHtWvXYufOnfjmm2+QkpKCgIAApKam6hw+IiICaWlp0ic5OdnQRSIiIqIqxuAtOCEhIdLf3t7e8Pf3R4MGDRAdHY3w8HCt4VUqFVQqlaGLQURERFWY0W8Tt7S0hLe3Ny5dumTsWREREREBKIOAk5WVhfPnz8PFxcXYsyIiIiICYISAM2XKFOzbtw9JSUk4dOgQ+vTpg/T0dAwZMsTQsyIiIiLSyeDX4Pzzzz/o168f7t27BwcHB7Rr1w4JCQlwd3c39KyIiIiIdDJ4wPnxxx8NPUkionLHJygTVS58FxURERHJDgMOERERyY5RnmRMRFQRleY0ExFVLgw4RFTu9L0aI6HuqDIuCRHJBU9RERERkeww4BAREZHsMOAQERGR7PAaHKrS9F37QURElRtbcIiIiEh2GHCIiIhIdniKiqoEnooiIqpa2IJDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssNXNZBei2MvlncRiIiISoQBR+YYUojKX2m2w0ldGxuwJERVB09RERERkeywBYeIqAJj6w9RyTDgkKy0u/F1eReBDEjf75lQd1QZl4SIKhueoiIiIiLZYcAhIiIi2WHAISIiItnhNThEVOnoujaH1+UQ0YvYgkNERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssOAQ0RERLLDgENERESyw4BDREREssMH/RERyRTfRE5VGVtwiIiISHbYgkNEsqDr9Q0AX+FAVFUx4BARkRae3qLKjgGnEijNjoaIiKgq4jU4REREJDtswaFKSd/1FkRERABbcIiIiEiGGHCIiIhIdniKqozwQmEiqip4BxZVBAw4RFQl8bk5FRPDERkKT1ERERGR7DDgEBERkewY7RTVihUr8Mknn+DWrVto3rw5lixZgo4dOxprdkXG5k8iIsqPxwb5MUrA2bhxI8LCwrBixQq0b98eX331FUJCQnDu3DnUrVvXGLMkGeCzbcgYDLVe6ZoOr9epWMrrZg6Go4rJKKeoFi1ahOHDh2PEiBHw9PTEkiVL4ObmhpUrVxpjdkREREQaDN6Ck52djaNHj2LatGka3YODgxEXF6c1fFZWFrKysqTvaWlpAID09HRDFw0AkPnkcYnHjdpyzIAlofyeZGQVPhCRkenbR+haP0uzPyECyu+4Mq5zQ4NPU33cFkIYfNolYfCAc+/ePeTm5sLJyUmju5OTE1JSUrSGj4qKwuzZs7W6u7m5GbpoRERFsMxIwxJVHB8acdqPHj2CjY2NEedQNEa7yFihUGh8F0JodQOAiIgIhIeHS9/z8vJw//592NnZ6Ry+rKWnp8PNzQ3JycmwtrYu7+LIBuvVOFivxsF6NQ7Wq3GUV70KIfDo0SO4urqW2TwLYvCAY29vDxMTE63Wmjt37mi16gCASqWCSqXS6FazZk1DF6vUrK2tuQEaAevVOFivxsF6NQ7Wq3GUR71WhJYbNYNfZGxmZoaXXnoJsbGxGt1jY2MREBBg6NkRERERaTHKKarw8HAMGjQIvr6+8Pf3x9dff40bN25gzJgxxpgdERERkQajBJy33noLqamp+Oijj3Dr1i14eXlh27ZtcHd3N8bsjEqlUmHWrFlap9GodFivxsF6NQ7Wq3GwXo2D9fqcQlSU+7mIiIiIDITvoiIiIiLZYcAhIiIi2WHAISIiItlhwCEiIiLZYcAhIiIi2alyAWfFihXw8PCAubk5XnrpJRw4cEDvsLdu3UL//v3RpEkTVKtWDWFhYVrDrFmzBgqFQuuTmZlpxKWoeIpTr5s3b0bXrl3h4OAAa2tr+Pv7Y+fOnVrD/fLLL2jWrBlUKhWaNWuGX3/91ZiLUCEZul65vj5XnHo9ePAg2rdvDzs7O1hYWKBp06ZYvHix1nBcXw1fr1xfnytOvb7or7/+glKpRMuWLbX6VYn1VVQhP/74ozA1NRXffPONOHfunHjvvfeEpaWluH79us7hk5KSxMSJE0V0dLRo2bKleO+997SGWb16tbC2tha3bt3S+FQlxa3X9957TyxYsEAcPnxYXLx4UURERAhTU1Nx7NgxaZi4uDhhYmIi5s2bJ86fPy/mzZsnlEqlSEhIKKvFKnfGqFeur8Wv12PHjon169eLM2fOiKSkJPH999+L6tWri6+++koahuurceqV62vx61Xt4cOHon79+iI4OFi0aNFCo19VWV+rVMBp27atGDNmjEa3pk2bimnTphU6bmBgoN6AY2NjY6ASVk6lqVe1Zs2aidmzZ0vf+/btK7p3764xTLdu3cTbb79dusJWIsaoV66vhqnX3r17i4EDB0rfub4ap165vpa8Xt966y0xY8YMMWvWLK2AU1XW1ypziio7OxtHjx5FcHCwRvfg4GDExcWVatqPHz+Gu7s76tSpg169euH48eOlml5lYoh6zcvLw6NHj2Brayt1i4+P15pmt27dSv1bVRbGqleA62tp6/X48eOIi4tDYGCg1I3rq3HqFeD6WpJ6Xb16Na5cuYJZs2bp7F9V1tcqE3Du3buH3NxcrTeaOzk5ab35vDiaNm2KNWvW4Pfff8eGDRtgbm6O9u3b49KlS6UtcqVgiHr97LPP8OTJE/Tt21fqlpKSYvDfqjIxVr1yfS15vdapUwcqlQq+vr4YN24cRowYIfXj+mqceuX6Wvx6vXTpEqZNm4Z169ZBqdT9Nqaqsr4a5V1UFZlCodD4LoTQ6lYc7dq1Q7t27aTv7du3R+vWrbF06VJ88cUXJZ5uZVPSet2wYQMiIyPx22+/wdHR0SDTlBND1yvX1+dKUq8HDhzA48ePkZCQgGnTpqFhw4bo169fqaYpN4auV66vzxW1XnNzc9G/f3/Mnj0bjRs3Nsg0K7MqE3Ds7e1hYmKilVDv3LmjlWRLo1q1amjTpk2V+Q+jNPW6ceNGDB8+HD/99BNeeeUVjX7Ozs5G/60qMmPVa35cX58rSr16eHgAALy9vXH79m1ERkZKB2Kur8ap1/y4vj6nr14fPXqEI0eO4Pjx4xg/fjyA56eqhRBQKpWIiYlB586dq8z6WmVOUZmZmeGll15CbGysRvfY2FgEBAQYbD5CCJw4cQIuLi4Gm2ZFVtJ63bBhA4YOHYr169ejZ8+eWv39/f21phkTE2PQ36oiM1a95sf19bni7geEEMjKypK+c301Tr3q6s/1VX+9Wltb4/Tp0zhx4oT0GTNmDJo0aYITJ07Az88PQBVaX8vhwuZyo77d7rvvvhPnzp0TYWFhwtLSUly7dk0IIcS0adPEoEGDNMY5fvy4OH78uHjppZdE//79xfHjx8XZs2el/pGRkWLHjh3iypUr4vjx42LYsGFCqVSKQ4cOlemylafi1uv69euFUqkUy5cv17j18+HDh9Iwf/31lzAxMRHz588X58+fF/Pnz5flbYwFMUa9cn0tfr0uW7ZM/P777+LixYvi4sWLYtWqVcLa2lpMnz5dGobrq3HqletryY5bL9J1F1VVWV+rVMARQojly5cLd3d3YWZmJlq3bi327dsn9RsyZIgIDAzUGB6A1sfd3V3qHxYWJurWrSvMzMyEg4ODCA4OFnFxcWW0NBVHceo1MDBQZ70OGTJEY5o//fSTaNKkiTA1NRVNmzYVv/zySxktTcVh6Hrl+vpccer1iy++EM2bNxfVq1cX1tbWolWrVmLFihUiNzdXY5pcXw1fr1xfnyvucetFugKOEFVjfVUIIUQ5NBwRERERGU2VuQaHiIiIqg4GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpIdBhwiIiKSHQYcIiIikh0GHCIiIpKd/wfJX4gCIFvybAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simulation_exp(n):\n",
    "    lm=4\n",
    "    samples=np.random.exponential(1/lm,n)\n",
    "    return np.mean(samples)\n",
    "\n",
    "def simulation_norm(n):\n",
    "     \n",
    "    samples=np.random.normal(0.25, 0.01**0.5,n)\n",
    "    return np.mean(samples)\n",
    "\n",
    "exp_means_exp=[simulation_exp(40) for i in range(1000)]\n",
    "exp_means_norm=[simulation_norm(40) for i in range(1000)]\n",
    "\n",
    "plt.hist(np.array(exp_means_exp),bins=30,density=True,label=r\"exp$(\\lambda=4)$\",alpha=0.5)\n",
    "plt.hist(np.array(exp_means_norm),bins=30,density=True,label=r\"norm$(\\mu=0.25, \\sigma^2=0.01)$\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"distribution of sample mean with size=40, Exp dist v.s. Norm dist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 ms ± 1.81 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "exp_means_exp=[simulation_exp(40) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 ms ± 273 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "exp_means_norm=[simulation_norm(40) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in class example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "import numpy as np\n",
    "import simpy\n",
    "\n",
    "from scipy.stats import uniform, poisson, expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:49<00:00,  1.01trial/s, best loss: -474.69]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n1': 3.0, 'n2': 2.0, 'n3': 2.0, 'n4': 5.0, 'n5': 4.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layer = 5 # mutilayer server\n",
    "\n",
    "lambdas = np.array([1, 2, 1, 2, 1, 2])\n",
    "\n",
    "def arrivals(env, servers, revenue):\n",
    "    N_t = poisson.rvs(100 * lambdas[0]) # total number of arrivals\n",
    "    arrival_times = uniform.rvs(size=N_t) * 100 # uniformly distributed arrival times over 100-unit time period\n",
    "    arrival_times = np.sort(np.insert(arrival_times, 0, 0)) # insert 0 at the begining\n",
    "    dt = arrival_times[1:] - arrival_times[:-1] # calculate for the each inter-arrival times\n",
    "    service_times = expon.rvs(size=(N_t, n_layer), scale=1 / lambdas[1:])\n",
    "    for i in range(0, len(dt)): # iterates through each arrival\n",
    "        yield env.timeout(dt[i])\n",
    "        env.process(service(env, servers, revenue, service_times[i,:]))\n",
    "\n",
    "\n",
    "def service(env, servers, revenue, st):\n",
    "    for i in range(0, len(st)):\n",
    "        rqt = servers[i].request()\n",
    "        yield rqt\n",
    "        yield env.timeout(st[i])\n",
    "        servers[i].release(rqt)\n",
    "        revenue.append(1)\n",
    "\n",
    "\n",
    "def simulate(n):\n",
    "    revenue = []\n",
    "    env = simpy.Environment()\n",
    "    servers = [simpy.Resource(capacity=n[i], env=env) for i in range(0, n_layer)] # n_layer servers\n",
    "    env.process(arrivals(env, servers, revenue))\n",
    "    env.run(until=100)\n",
    "    return np.sum(revenue) - n[0] * 2 - n[1] * 1 - n[2] * 2 - n[3]* 0.5 - n[4] * 1.5 # every layer server has different revenue\n",
    "\n",
    "def run_simulations(n):\n",
    "    np.random.seed(42)\n",
    "    X = [simulate(n) for i in range(100)]\n",
    "    return (-1) * np.mean(X)\n",
    "\n",
    "hyperopt.fmin(fn=run_simulations,\n",
    "    space=[hyperopt.hp.quniform('n1', 1, 10, 1),\n",
    "           hyperopt.hp.quniform('n2', 1, 10, 1),\n",
    "           hyperopt.hp.quniform('n3', 1, 10, 1),\n",
    "           hyperopt.hp.quniform('n4', 1, 10, 1),\n",
    "           hyperopt.hp.quniform('n5', 1, 10, 1),\n",
    "           ],\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Antithetic Method\n",
    "\n",
    "## Overview\n",
    "In this lecture, we study how to use anti-thetic method for variance reduction. \n",
    "\n",
    "Learning objective:\n",
    "\n",
    "Understand how antithetical method and control variates can be applied\n",
    "\n",
    "## Antithetic method\n",
    "Antithetic  method can be used to find a distribution that has the same mean but with a much smaller variance comparing to the distribution of our original system output. \n",
    "\n",
    "For the regular method, we generate n independent samples of random variable X to get x1,x2,x3,...,xn. We then use $\\sum \\limits _{i=1} ^ {n} X_{i}$ estimate the populate mean.\n",
    "\n",
    "Antithetic method proposes that us to find variable X that satisfies\n",
    "\n",
    "- E(X') = E(X)\n",
    "- Var(X') = Var(X)\n",
    "- Cov(X',X) < 0\n",
    "\n",
    "We can generate ​$n/2$ independent samples of $Y=(X'+X)/2$ to get y1,y2,...,yn/2. We then use $2/n\\sum \\limits _{i=1} ^ {n/2} y_{i}$ estimate the populate mean. \n",
    "\n",
    "We can use convolution to get the sampling done\n",
    "- generate $x_1,x_2,...,x_{n/2}$\n",
    "- generate $x_1',x_2',...,x_{n/2}'$\n",
    "- compute $y_1 = (x_1 + x_1')/2, (x_2 + x_2')/2,...,(x_{n/2} + x_{n/2}')/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing X'\n",
    "\n",
    "If X = h(U1,U2,U3,...), where U1,U2,U3,... all follow a uniform distribution between 0 and 1, then X' = h(1-U1,1-U2,1-U3,...) might be such a candidate.\n",
    "\n",
    "To see why this is the case, we need to check our assumptions\n",
    "\n",
    "- U1,U2,U3,...all follow a uniform distribution. 1-U1,1-U2,1-U3,...will follow a uniform distribution between 0 and 1 as well. As a result, X and X′ both follow the same distribution, which gives us E(X) and E(X') and Var(X) = Var(X')\n",
    "\n",
    "- $Cov(U_i,1-U_i) = E(U_i*(1-U_i)) - E(U_i)E(1-U_i) = E(U_i) - E(U_i)^2 - E(U_i)E(1-U_i) = 0.5 -Var(U_i) - E(U_i)^2 - 0.25 = -Var(U_i) = -1/12 $\n",
    "    - We used property that $Var(x) = E(x^2) - E(x)^2$\n",
    "\n",
    "Since all the inputs are negatively correlated, it is very likely the output X and \n",
    "X′ are negatively correlated as well. This is especially true if h is a monotone (either increasing or decreasing) function with each of its coordinates.  Based on our discussion above, we can use the following step to perform antithetic variance reduction. Thus, we can \n",
    "- sample $u_1,u_2,...u_{n/2}$ to generate samples of X\n",
    "- sample $u_1',u_2',...u_{n/2}'$ to generate samples of X'\n",
    "- compute $y_i=(x_i+x_i')/2$\n",
    "- sample $y_1,y_2,...y_{n/2}$ to generate samples of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1:    \n",
    "Estimating the mean of an exponential distribution.  Notice that u and X shared a monotonic relationship. \n",
    "\n",
    "In this demo, we use s^2/n to estimate the variance of the sample mean. Here, s^2 is the sample variance, while n is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8628211766369721 0.008832428634632856\n",
      "0.9237412894101782 0.0016305204179539636\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#sample size 100\n",
    "size1=100\n",
    "lmbda=1\n",
    "x=-1/lmbda*np.log(np.random.rand(size1)) # exponential random variables\n",
    "sm_var=np.var(x,ddof=1)/(size1)\n",
    "sm=np.mean(x)\n",
    "print(sm,sm_var)\n",
    "\n",
    "\n",
    "#sample size 50\n",
    "size2=int(size1/2)\n",
    "u=np.random.rand(size2)\n",
    "x_original=-1/lmbda*np.log(u)\n",
    "x_anti=-1/lmbda*np.log(1-u)\n",
    "y=(x_original+x_anti)/2\n",
    "\n",
    "sm_var1=np.var(y,ddof=1)/(size2)\n",
    "sm1=np.mean(y)\n",
    "print(sm1,sm_var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2\n",
    "When X is generated from a normal distribution N(μ,σ^2), 2μ−x will be from the same normal distribution and negatively correlated with x. \n",
    "\n",
    "Samples from a normal distribution are also converted from u. If we cannot recover u in this case, we could use the fact that if x is converted from u, then 2μ−x is converted from 1−u.\n",
    "\n",
    "Thus, we can use x and 2μ−x as pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size1=100\n",
    "x=np.random.normal(5,1,size1)\n",
    "sm_var=np.var(x,ddof=1)/(size1)\n",
    "sm=np.mean(x)\n",
    "print(sm,sm_var)\n",
    "\n",
    "size2=50\n",
    "x_original=np.random.normal(5,1,size2)\n",
    "x_anti=10-x_original\n",
    "y=(x_original+x_anti)/2\n",
    "sm_var1=np.var(y,ddof=1)/(size2)\n",
    "sm1=np.mean(y)\n",
    "print(sm1,sm_var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension\n",
    "Using scipy to help find the original part and antithetic part.\n",
    "\n",
    "For the common distributions that have been programmed in scipy.stats , we can use the following two steps to find the antithetic part:\n",
    "\n",
    "use u=np.random.rand() to a random number\n",
    "\n",
    "use ppf(u) and ppf(1-u) function to find out the corresponding x and x′ sample \n",
    "\n",
    "For example,  the following method can generate samples for original and antithetic of Poisson samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.18 0.14674343434343431\n",
      "20.0 0.0016326530612244896\n",
      "[19. 22. 23. 22. 17. 15. 33. 16. 18. 19. 17. 27. 18. 19. 15. 15. 17. 29.\n",
      " 20. 20. 23. 22. 17. 16. 13. 21. 20. 19. 22. 20. 20. 17. 15. 12. 18. 17.\n",
      " 22. 16. 21. 24. 16. 16. 20. 28. 19. 24. 17. 30. 17. 20.] [20. 18. 17. 18. 22. 25.  9. 24. 22. 21. 23. 13. 22. 21. 25. 25. 23. 12.\n",
      " 20. 19. 17. 18. 22. 24. 27. 19. 20. 20. 18. 19. 19. 23. 26. 29. 22. 23.\n",
      " 18. 24. 19. 16. 24. 24. 20. 13. 21. 16. 23. 11. 23. 20.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as spst\n",
    "\n",
    "##original method \n",
    "lmbda=20\n",
    "size1=100\n",
    "x=np.random.poisson(lmbda,size1)\n",
    "sm_var=np.var(x,ddof=1)/(size1)\n",
    "sm=np.mean(x)\n",
    "print(sm,sm_var)\n",
    "\n",
    "##antithetic method \n",
    "lmbda=20\n",
    "size2=int(size1/2)\n",
    "u=np.random.rand(size2)\n",
    "x_original=spst.poisson.ppf(u,lmbda)\n",
    "x_anti=spst.poisson.ppf(1-u,lmbda)\n",
    "\n",
    "y=(x_original+x_anti)/2\n",
    "sm_var1=np.var(y,ddof=1)/(size2)\n",
    "sm1=np.mean(y)\n",
    "print(sm1,sm_var1)\n",
    "print(x_original,x_anti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying antithetic method to system simulation\n",
    "the basic single queueing system, we have two sets of samples to feed into the system\n",
    "\n",
    "- (1) the inter-arrival time for each customer\n",
    "\n",
    "- (2) the service time for each customer\n",
    "\n",
    "Let's assume that we are interested in measuring the average time consumers spend in the system. We call this our system output.\n",
    "\n",
    "To make sure that anti-thetic method is applied, we need to make sure that \n",
    "\n",
    "- In each round of the simulation, we need to make sure that the inter-arrival times and the service time as still generated from the same distribution. This makes sure that our system output is still generated from the same distribution\n",
    "\n",
    "- We need to make sure that if one round of simulation gives a large system output, we need to make sure that the next round will, in general, give us a smaller system output. Vice versa. \n",
    "\n",
    "If the samples are generated using the inverse transform method or from a normal distribution, it will be very easy to generate the original parts and the anti-thetic parts using the technique we discussed in class. We need to think about whether feeding the original part and the anti-thetic part will lead to a negative correlation in the actual system output.\n",
    "\n",
    "(1) larger inter-arrival time v.s. smaller inter-arrival time\n",
    "\n",
    "Having larger inter-arrival time means consumers in this round of simulation are arrival at a lower frequency. In this case, we might be expecting fewer people to line up, which will lead to a lower average time in the system. \n",
    "\n",
    "If the inter-arrival times fed into the system are generally large, we should expect the antithetic part to be generally small. Thus, we will be able to produce negatively correlated system outputs.\n",
    "\n",
    "(2) larger service time v.s. smaller service time\n",
    "\n",
    "Larger service time has two impacts\n",
    "\n",
    "- a. More people might line up. Thus, consumers might wait for a longer time before getting the service\n",
    "\n",
    "- b. The actual service time is part of the time in the system for a consumer. Longer service time will directly contribute to a longer time in the system.\n",
    "\n",
    "Thus, having when service time for two rounds of simulating being negatively correlated can also help to generate negatively correlated system output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm:\n",
    "- If we want to apply anti-thetic method to system input 1 and 2 among all the inputs, then we need to repeat the following multiple times\n",
    "\n",
    "    - Generate R1, R2 random states\n",
    "\n",
    "    - Copy R1, R2 random states to get R1c, R2c \n",
    "\n",
    "    - Use R1, R2 to generate the random numbers used for the original round of simulation, which produces output x original\n",
    "\n",
    "    - Use R1c, R2c to generate the random numbers used for the original round of simulation, which produces output x anti\n",
    "​\n",
    "    - compute the average of the two y = (x original +x anti)/2\n",
    "​\n",
    "- construct the sample mean and confidence interval using all the y's collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import simpy\n",
    "\n",
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53524935, 0.03341483])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1 = expon.rvs(scale=scale)\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88073845, 3.41541622])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = expon.ppf(1 - expon.cdf(dt1, scale=scale), scale=scale)\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## antithetic samples\n",
    "\n",
    "lmbda_1 = 1.\n",
    "lmbda_2 = 1.\n",
    "\n",
    "def arrivals(env, server, T, anti):\n",
    "    scale = np.power([lmbda_1, lmbda_2],-1) # 倒数\n",
    "    for i in range(0, 100):\n",
    "        dt = expon.rvs(scale=scale) # expon: intterarrival in poisson process\n",
    "        if anti == True:\n",
    "            dt = expon.ppf(1 - expon.cdf(dt, scale=scale), scale=scale)\n",
    "            # dt\n",
    "            # expon.cdf(dt, scale=scale): the probability that less than or equal to dt\n",
    "            # 1 - expon.cdf(dt, scale=scale): complement of cdf -> prob that greater than dt\n",
    "            # expon.ppf(1 - expon.cdf(dt, scale=scale), scale=scale): the value that cdf equal to the complement\n",
    "            # -> the opposite of the original dt\n",
    "        yield env.timeout(dt[0])\n",
    "        env.process(service(env, server, dt, T))\n",
    "\n",
    "def service(env, server, dt, T):\n",
    "    t0 = env.now\n",
    "    rqt = server.request()\n",
    "    yield rqt\n",
    "    yield env.timeout(dt[1])\n",
    "    server.release(rqt)\n",
    "    t1 = env.now\n",
    "    T.append(t1 - t0)\n",
    "    \n",
    "\n",
    "def simulate(anti):\n",
    "    T = []\n",
    "    env = simpy.Environment()\n",
    "    server = simpy.Resource(env, capacity=1)\n",
    "    env.process(arrivals(env, server, T, anti))\n",
    "    env.run()\n",
    "    return np.mean(T)\n",
    "\n",
    "def eval_mean(n, seed):\n",
    "    np.random.seed(seed)\n",
    "    T_1 = [simulate(False) for i in range(0, n)]\n",
    "    np.random.seed(seed)\n",
    "    T_2 = [simulate(True) for i in range(0, n)]\n",
    "    T_1 = np.array(T_1)\n",
    "    T_2 = np.array(T_2)\n",
    "    T_means = (T_1 + T_2) / 2\n",
    "    return np.mean(T_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.879790550213511"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_mean(100, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Common Random Numbers\n",
    "\n",
    "## Overview\n",
    "In this lecture, we discuss how to use the common random number method to reduce variance when we need to compare differen policies.\n",
    "\n",
    "Learning objective:\n",
    "\n",
    "- Understand how and when common random numbers strategy can be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common random number algorithm\n",
    "Common random number algorithm is a commonly used variance reduction technique when comparing different policies. \n",
    "\n",
    "Let's assume that we have configuration 1 and 2 (for example one server v.s. two servers). On ith round of simulation, each configuration generates its respective system output sample of X1i and X2i respectively. We are interested in comparing different difference between the system outputs due to two different configurations. Thus, we compute run each configuration N rounds independently to get "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can find a way to introduce a positive correlation between x 1i and x 2i, then we achieve variance reduction.\n",
    "\n",
    "Answer: common random numbers might do the trick. \n",
    "\n",
    "Reason: if two settings have the same system input (determined by u), then under two different settings the outcomes might both tend to tilt towards the same direction.\n",
    "\n",
    "For example, we might be interested in comparing the difference in the average time in the system under one-server and two-server. When we use the same set of service times and the arrival times for both simulations, the system output might tilt toward the same direction. For example, if ithe iteration gives very large inter-arrival times and short service times, no matter whether we use one server or two servers, the average time in the system will both be very low, thus a positive correlation.\n",
    "\n",
    "Notice that common random numbers might not always induce positive correlations. If negative correlations are induced, we will see a variance increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Assume that Y1 = log(X) and Y2 = log(X+1), where X~Exp(0.2). Estimate E(Y1) - E(Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5130305690679271 0.04793979114309394\n",
      "-0.49752800712812295 0.01957605746717936\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "#no common random number\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X=-1/lm*np.log(U)\n",
    "Y1=np.log(X)\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X=-1/lm*np.log(U)\n",
    "Y2=np.log(X+1)\n",
    "print(np.mean(Y1)-np.mean(Y2),np.std(Y1-Y2,ddof=1)/1000**0.5)\n",
    "\n",
    "\n",
    "##with common random variable\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X=-1/lm*np.log(U)\n",
    "Y1=np.log(X)\n",
    "Y2=np.log(X+1) # y1, y2 positive correlated\n",
    "print(np.mean(Y1)-np.mean(Y2),np.std(Y1-Y2,ddof=1)/1000**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:  Assume that Y1 = log(X) and Y2 = -log(X+1), where X~Exp(0.2). Estimate E(Y1) - E(Y2). In this example, Y1 and Y2 are negatively correlated if we use common random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5203170223220233 0.04589849017860493\n",
      "2.534726289599544 0.062378017490919986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "#no common random number\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X=-1/lm*np.log(U)\n",
    "Y1=np.log(X)\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X=-1/lm*np.log(U)\n",
    "Y2=-np.log(X+1)\n",
    "print(np.mean(Y1)-np.mean(Y2),np.std(Y1-Y2,ddof=1)/1000**0.5)\n",
    "\n",
    "\n",
    "##with common random variable\n",
    "##this one is bad because using common random numbers \n",
    "#produced a negative correlation\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X=-1/lm*np.log(U)\n",
    "Y1=np.log(X)\n",
    "Y2=-np.log(X+1)\n",
    "print(np.mean(Y1)-np.mean(Y2),np.std(Y1-Y2,ddof=1)/1000**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5930015101530657 0.04709807031871637\n",
      "2.553255743549584 0.017181483567398726\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "#no common random number\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X=-1/lm*np.log(U)\n",
    "Y1=np.log(X)\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X=-1/lm*np.log(U)\n",
    "Y2=-np.log(X+1)\n",
    "print(np.mean(Y1)-np.mean(Y2),np.std(Y1-Y2,ddof=1)/1000**0.5)\n",
    "\n",
    "##with common random variable\n",
    "U=np.random.rand(1000)\n",
    "lm=0.2\n",
    "X1=-1/lm*np.log(U)\n",
    "X2=-1/lm*np.log(1-U) # x1, x2 negative correlated\n",
    "Y1=np.log(X1)\n",
    "Y2=-np.log(X2+1) # y1, y2 positive correlated\n",
    "print(np.mean(Y1)-np.mean(Y2),np.std(Y1-Y2,ddof=1)/1000**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "import numpy as np\n",
    "import simpy\n",
    "\n",
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without variance reduction\n",
    "\n",
    "lmbda = [20, 10, 30, 10]\n",
    "\n",
    "def arrivals(env, servers,T):\n",
    "    while True:\n",
    "        dt = expon.rvs(scale=1/lmbda[0])\n",
    "        if env.now + dt > 1:\n",
    "            break\n",
    "        yield env.timeout(dt)\n",
    "        env.process(service(env, servers, T))\n",
    "\n",
    "        \n",
    "def service(env,servers, T): # 依次经过三个处理器\n",
    "    t0 = env.now\n",
    "    for i, server in enumerate(servers):\n",
    "        rqt = server.request()\n",
    "        yield rqt\n",
    "        dt = expon.rvs(scale=1/lmbda[i+1])\n",
    "        yield env.timeout(dt)\n",
    "        server.release(rqt)\n",
    "    t1 = env.now\n",
    "    T.append(t1-t0)\n",
    "\n",
    "def simulate(n1, n2):\n",
    "    T = []\n",
    "    env = simpy.Environment()\n",
    "    servers = [simpy.Resource(env, capacity=n1),\n",
    "               simpy.Resource(env, capacity=n2),\n",
    "               simpy.Resource(env, capacity=20 - n2 - n1)]\n",
    "    env.process(arrivals(env, servers, T))\n",
    "    env.run()\n",
    "    return np.mean(T)\n",
    "\n",
    "def eval_mean(n):\n",
    "    n1 = n[0]\n",
    "    n2 = n[1]\n",
    "    X = [simulate(n1,n2) for i in range(0, 100)]\n",
    "    return np.mean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.90trial/s, best loss: 0.22936534918010282]\n",
      "{'n1': 7.0, 'n2': 5.0}\n"
     ]
    }
   ],
   "source": [
    "best = hyperopt.fmin(fn=eval_mean,\n",
    "    space=[hyperopt.hp.quniform('n1', 1, 9, 1),hyperopt.hp.quniform('n2', 1, 9,1)],\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=50)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3395208142060733,\n",
       "  0.25515099181120454,\n",
       "  0.2416810506323877,\n",
       "  0.23647641188947519,\n",
       "  0.23509093674528408,\n",
       "  0.2304365022197428,\n",
       "  0.23799314706813468,\n",
       "  0.23462491866914992],\n",
       " [0.34520937027027004,\n",
       "  0.2495212622603207,\n",
       "  0.23864117919251754,\n",
       "  0.23069516517298794,\n",
       "  0.2346818052667881,\n",
       "  0.2342372560480877,\n",
       "  0.24287807644188167,\n",
       "  0.2329225688120939],\n",
       " [0.32797403690143795,\n",
       "  0.25314089389722194,\n",
       "  0.23970179732418123,\n",
       "  0.23511319790496,\n",
       "  0.2359678846192341,\n",
       "  0.22918641421198896,\n",
       "  0.23040274650759585,\n",
       "  0.2351246129661397],\n",
       " [0.3624007789818875,\n",
       "  0.2552977311309171,\n",
       "  0.2396646287421201,\n",
       "  0.23334170089162087,\n",
       "  0.23020641892766686,\n",
       "  0.2411415678341837,\n",
       "  0.23503480086696324,\n",
       "  0.22987722834262617],\n",
       " [0.33288627511910207,\n",
       "  0.24926286076338525,\n",
       "  0.235558111242966,\n",
       "  0.23318988050508796,\n",
       "  0.2303333954142138,\n",
       "  0.2353086852278555,\n",
       "  0.23456762171026527,\n",
       "  0.238408376057428],\n",
       " [0.32443035349079863,\n",
       "  0.2544005119682958,\n",
       "  0.2356406941735088,\n",
       "  0.22578308832473115,\n",
       "  0.23240651414913066,\n",
       "  0.22762764052509316,\n",
       "  0.2385690669127915,\n",
       "  0.2418885074707191],\n",
       " [0.31190134668932507,\n",
       "  0.24572277956599958,\n",
       "  0.23529477634212964,\n",
       "  0.2379527327617499,\n",
       "  0.23561754489049466,\n",
       "  0.23727922072249946,\n",
       "  0.23927240304014372,\n",
       "  0.24998484405466542],\n",
       " [0.33833383679069023,\n",
       "  0.2605207873387377,\n",
       "  0.23617381643179697,\n",
       "  0.23087112725738812,\n",
       "  0.23152937655265565,\n",
       "  0.23995570752653955,\n",
       "  0.2622820773066083,\n",
       "  0.35308219412795694]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[eval_mean([i,j]) for i in range(2,10)] for j in range(2,10)]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with common random numbers\n",
    "\n",
    "lmbda = [20., 10., 30., 10.]\n",
    "\n",
    "def arrivals(env, servers,T):\n",
    "    i = 0\n",
    "    while True:\n",
    "        dt = expon.rvs(scale=np.power(lmbda,-1)) # 一次性生成随机数，所以在每个server里用到的都是这一组\n",
    "        if env.now + dt[0] > 1:\n",
    "            break\n",
    "        yield env.timeout(dt[0])\n",
    "        env.process(service(env, servers, T, dt, i))\n",
    "        i = i + 1\n",
    "\n",
    "        \n",
    "def service(env,servers, T, dt, i):\n",
    "    t0 = env.now\n",
    "    for j, server in enumerate(servers):\n",
    "        rqt = server.request()\n",
    "        yield rqt\n",
    "        yield env.timeout(dt[j+1])\n",
    "        server.release(rqt)\n",
    "    t1 = env.now\n",
    "    T.append(t1-t0)\n",
    "\n",
    "def simulate(n1, n2):\n",
    "    T = []\n",
    "    env = simpy.Environment()\n",
    "    servers = [simpy.Resource(env, capacity=n1),\n",
    "               simpy.Resource(env, capacity=n2),\n",
    "               simpy.Resource(env, capacity=(20 - n2 - n1))]\n",
    "    env.process(arrivals(env, servers, T))\n",
    "    env.run()\n",
    "    return np.mean(T)\n",
    "\n",
    "def eval_mean(n):\n",
    "    np.random.seed(42)\n",
    "    n1 = n[0]\n",
    "    n2 = n[1]\n",
    "    X = [simulate(n1, n2) for i in range(0, 100)]\n",
    "    return np.mean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:07<00:00,  3.16trial/s, best loss: 0.22935072235676948]\n",
      "{'n1': 8.0, 'n2': 4.0}\n"
     ]
    }
   ],
   "source": [
    "best = hyperopt.fmin(fn=eval_mean,\n",
    "    space=[hyperopt.hp.quniform('n1', 1, 9, 1),hyperopt.hp.quniform('n2', 1, 9,1)],\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=25)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3360132492314921,\n",
       "  0.25447533697451874,\n",
       "  0.23632519330049384,\n",
       "  0.23288108108178193,\n",
       "  0.2324145820939136,\n",
       "  0.2322144442779959,\n",
       "  0.23221473738464232],\n",
       " [0.3340772072468707,\n",
       "  0.25202932048415805,\n",
       "  0.23390436651070823,\n",
       "  0.23036630254999468,\n",
       "  0.22976537043935086,\n",
       "  0.22956808701052542,\n",
       "  0.229538952878036],\n",
       " [0.3339256916283854,\n",
       "  0.25183542192638453,\n",
       "  0.23370431981029202,\n",
       "  0.23017484971307925,\n",
       "  0.2295501787731977,\n",
       "  0.22936886763417325,\n",
       "  0.22935072235676948],\n",
       " [0.33391193894574045,\n",
       "  0.25182580834807244,\n",
       "  0.23369197243751486,\n",
       "  0.23016141606781917,\n",
       "  0.229536191195917,\n",
       "  0.22936534918010282,\n",
       "  0.22937031491100787],\n",
       " [0.33391193894574045,\n",
       "  0.25182580834807244,\n",
       "  0.23369197243751486,\n",
       "  0.23016425474923763,\n",
       "  0.22954764173849831,\n",
       "  0.22939911987006828,\n",
       "  0.2294984377783414],\n",
       " [0.33391193894574045,\n",
       "  0.25182580834807244,\n",
       "  0.2336948111189333,\n",
       "  0.23017570529181897,\n",
       "  0.22957756172715624,\n",
       "  0.2295272427374018,\n",
       "  0.23032410482117194],\n",
       " [0.33391193894574045,\n",
       "  0.25182580834807244,\n",
       "  0.23370626166151465,\n",
       "  0.2302065127906392,\n",
       "  0.22970939323079662,\n",
       "  0.230358131330268,\n",
       "  0.23426414710925755]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[eval_mean([i,j]) for i in range(2,9)] for j in range(2,9)]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Control Variate\n",
    "\n",
    "## Overview\n",
    "In this lecture, we study how to use variance reduction to improve the efficiency of the simulation.\n",
    "\n",
    "Course objective:\n",
    "\n",
    "- Understand when variance reduction technique should be used\n",
    "\n",
    "- Understand how antithetical method and control variates can be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control variate method\n",
    "\n",
    "Again, our task is to reduce Var(X).\n",
    "\n",
    "Suppose we can easily generate a variable Z that with a known mean μz and correlated with X. \n",
    "\n",
    "Can we use Z to help us achieve variance reduction?\n",
    "\n",
    "Thus, there are three key points we are looking for:\n",
    "\n",
    "1. Z is easy to generate \n",
    "\n",
    "2. E(Z) has a closed-form solution\n",
    "\n",
    "3. Z and X are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "- Step 1: Generate a set of x samples and also a set of z samples.\n",
    "\n",
    "- Step 2: estimate $c=-cov(x,z)/var(z)$ based on the samples we have\n",
    "\n",
    "- Step 3: Generate y samples using x+c(z−μz)\n",
    "\n",
    "- Step 4: Construct the confidence interval using the y samples\n",
    "\n",
    "Extension: \n",
    "\n",
    "- In practice, it is very likely that we do not know Cov(x,z). One possible solution is to estimate this correlation based on the simulation results. This c might make the Var(Y) slightly larger. However, we should still gain from the efficiency. \n",
    "\n",
    "- Remember, μ should use the theoretical mean, not the sample mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying control variates method to system simulation\n",
    "Again,  Let's assume that we are still interested in constructing the confidence interval for the system output for the same system we studied before.\n",
    "\n",
    "How do we find the variables that are correlated with the average time?\n",
    "\n",
    "Many options:\n",
    "\n",
    "Average inter-arrival time: This should be negatively correlated with the system output. The expected value should be equal to 1/10. Also, this value can be easily computed.\n",
    "\n",
    "Average service time: This should be positively correlated with the system output. The expected value should be equal to 1/11. Also, this value can be easily computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including several control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import simpy\n",
    "\n",
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.991927811598808"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## control variates\n",
    "\n",
    "lmbda_1 = 1.\n",
    "lmbda_2 = 1.\n",
    "\n",
    "def arrivals(env, server, T, IT, ST):\n",
    "    scale = np.power([lmbda_1, lmbda_2],-1)\n",
    "    for i in range(0, 100):\n",
    "        dt = expon.rvs(scale=scale)\n",
    "        IT.append(dt[0])\n",
    "        ST.append(dt[1])\n",
    "        yield env.timeout(dt[0])\n",
    "        env.process(service(env, server, dt, T))\n",
    "\n",
    "def service(env, server, dt, T):\n",
    "    t0 = env.now\n",
    "    rqt = server.request()\n",
    "    yield rqt\n",
    "    yield env.timeout(dt[1])\n",
    "    server.release(rqt)\n",
    "    t1 = env.now\n",
    "    T.append(t1 - t0)\n",
    "    \n",
    "def simulate():\n",
    "    T = []\n",
    "    IT = []\n",
    "    ST = []\n",
    "    env = simpy.Environment()\n",
    "    server = simpy.Resource(env, capacity=1)\n",
    "    env.process(arrivals(env, server, T, IT, ST))\n",
    "    env.run()\n",
    "    return np.mean(T), np.mean(IT), np.mean(ST)\n",
    "\n",
    "def eval_mean(n):\n",
    "    T_means = np.zeros(n)\n",
    "    IT_means = np.zeros(n)\n",
    "    ST_means = np.zeros(n)\n",
    "    for i in range(0, n):\n",
    "        t, it, st = simulate()\n",
    "        T_means[i] = t\n",
    "        IT_means[i] = it\n",
    "        ST_means[i] = st\n",
    "    c1 = -np.cov(T_means, IT_means)[0][1] / np.var(IT_means)\n",
    "    c2 = -np.cov(T_means, ST_means)[0][1] / np.var(ST_means)\n",
    "    T_tilde = T_means + c1 * (IT_means - 1 / lmbda_1) + c2 * (ST_means - 1 / lmbda_2)\n",
    "    return np.mean(T_tilde)\n",
    "\n",
    "eval_mean(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Variance Reduction by Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An insurance example\n",
    "\n",
    "Suppose an insurance company has a very large number of policies, and the probability of a policy making a claim in a given year is small. Then the number of claims in that year will be approximately Poisson distributed. Let's further suppose that the size of each claim is an independent and identically distributed positive random variable.\n",
    "\n",
    "Then the size of the total claims in a given year can be thought as: $\\sum \\limits _{j=1} ^ {N} X_{j}$  where X∼g where g is some arbitrary positive distribution, and N∼Poisson(λ) where λ is the expected number of claims. This is called a mixed Poisson distribution. Depending on what g is, calculating the closed form solution for this will be hard.\n",
    "\n",
    "We can sample from S_i according to the following procedure:\n",
    "\n",
    "(1) Generate Ni\n",
    "​\n",
    "\n",
    "(2) Generate Xi,1 ,...,Xi,Ni, and take Si to be the sum of those.\n",
    "\n",
    "Suppose we want to estimate $p_c=P(S>c)$. If c is large, this will presumably be a quite small; however, being able to accurately estimate it might be important (for the insurance company's capital requirements, etc).\n",
    "\n",
    "We might naively calculate this probability using $f(S)=1(S>C)$, and using $theta = 1/n \\sum \\limits _{i=1} ^ {n} f(S_i)$\n",
    "\n",
    "However, this will have variance $p_c(1-p_c)/n$ which will potentially be quite largely relative to the true value of p_c; our percentage error will likely be quite large.\n",
    "\n",
    "Let's consider the following alternative approach:\n",
    "\n",
    "(1) We generate  $X_i,j$ until $\\sum \\limits _{j=1}^{} kX_i,j > c$\n",
    "\n",
    "(2) Let Mi = min $ until $\\sum \\limits _{j=1}^{} kX_i,j > c$\n",
    "\n",
    "(3) Repeat this procedure for i = 1,..., n\n",
    "\n",
    "Further more, we can note  $\\sum \\limits _{j=1}^{N} X_j > c$ only if $N>=M$, hence: $E[f(S)|M=m] = P(N>=m)$ which 1 = F_lambda(m-1) whre F_lambda is the corresponding poisson cdf.\n",
    "\n",
    "Actually, we can improve this even more, by adding a control variate:\n",
    "\n",
    "Let $Y_i=\\sum \\limits _{j=1}^{N} (X_i,j - \\mu)$; it can be shown that this has expectation zero. Moreover, it will have a large magnitude of correlation with M: if M is large, then the X_i's should have been small, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.000999499874937461\n",
      "0.000323238200240464\n",
      "0.0001162288467788396\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import poisson, expon\n",
    "\n",
    "# naive\n",
    "\n",
    "\n",
    "lmbda = 10\n",
    "mu = 10\n",
    "c = 300\n",
    "\n",
    "def sample_S(n):\n",
    "    S= np.zeros(n)\n",
    "    N = poisson.rvs(lmbda, size=n)\n",
    "    for i in range(0, n):\n",
    "        X = expon.rvs(scale=mu, size=N[i])\n",
    "        S[i] = np.sum(X)\n",
    "    return S\n",
    "\n",
    "n = 1_000\n",
    "\n",
    "S = sample_S(n)\n",
    "f = (S > c).astype(int)\n",
    "print(np.mean(f))\n",
    "print(np.std(f)/np.sqrt(n))\n",
    "\n",
    "def sample_M(n):\n",
    "    M = np.zeros(n)\n",
    "    for i in range(0, n):\n",
    "        S_tmp = 0\n",
    "        j = 0\n",
    "        while S_tmp <= c:\n",
    "            S_tmp += expon.rvs(scale=mu) \n",
    "            j = j + 1\n",
    "        M[i] = j\n",
    "    return M\n",
    "\n",
    "def E_F_M(M):\n",
    "    return 1 - poisson.cdf(M-1, lmbda)\n",
    "\n",
    "M = sample_M(n)\n",
    "efm = E_F_M(M)\n",
    "\n",
    "print(np.mean(efm))\n",
    "print(np.std(efm)/np.sqrt(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A queueing example\n",
    "\n",
    "Suppose we want to estimate the total time in system of the first n customers in a queuing system.\n",
    "\n",
    "Suppose Wi is the time in system for the ith customer.\n",
    "\n",
    "Then θ=E[∑Wi]. \n",
    "\n",
    "Suppose Si is the state of the system when customer i arrives.\n",
    "\n",
    "Then: $\\sum \\limits _{i=1} ^ {n} E[W_I|S_i]$ will be an unbiased estimator θ with less variance (assuming we can calculate $E[W_I|S_i]$ explicitly).\n",
    "\n",
    "Let's look at the more specific example of a single-server system with exp(λ) service times. Then Si = Ni - the number of people in the system when person i arrives. Then, $E[W_I|S_i] = (1+N_i)/\\lambda$\n",
    "\n",
    " since the Ni people in front of person i need to be service in addition to person i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45496097166730887\n",
      "0.030965919817678416\n",
      "0.4482\n",
      "0.017611575738700952\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import simpy\n",
    "\n",
    "from scipy.stats import expon\n",
    "\n",
    "## naive way\n",
    "\n",
    "lmbda_1 = 4.\n",
    "lmbda_2 = 5.\n",
    "\n",
    "np.random.seed(42)\n",
    "def arrivals(env, server, T):\n",
    "    for i in range(0, 10):\n",
    "        dt = expon.rvs(scale=np.power([lmbda_1, lmbda_2],-1))\n",
    "        yield env.timeout(dt[0])\n",
    "        env.process(service(env, server, dt, T))\n",
    "\n",
    "def service(env, server, dt, T):\n",
    "    t0 = env.now\n",
    "    rqt = server.request()\n",
    "    yield rqt\n",
    "    yield env.timeout(dt[1])\n",
    "    server.release(rqt)\n",
    "    t1 = env.now\n",
    "    T.append(t1 - t0)\n",
    "    \n",
    "\n",
    "def simulate():\n",
    "    T = []\n",
    "    env = simpy.Environment()\n",
    "    server = simpy.Resource(env, capacity=1)\n",
    "    env.process(arrivals(env, server, T))\n",
    "    env.run()\n",
    "    return np.mean(T)\n",
    "\n",
    "n = 100\n",
    "T = [simulate() for i in range(0, n)]\n",
    "print(np.mean(T))\n",
    "print(np.std(T)/np.sqrt(n))\n",
    "\n",
    "## with variance reduction\n",
    "\n",
    "lmbda_1 = 4.\n",
    "lmbda_2 = 5.\n",
    "\n",
    "np.random.seed(42)\n",
    "def arrivals(env, server, N):\n",
    "    for i in range(0, 10):\n",
    "        dt = expon.rvs(scale=np.power([lmbda_1, lmbda_2],-1))\n",
    "        yield env.timeout(dt[0])\n",
    "        N.append(len(server.queue) + server.count)\n",
    "        env.process(service(env, server, dt))\n",
    "\n",
    "def service(env, server, dt):\n",
    "    rqt = server.request()\n",
    "    yield rqt\n",
    "    yield env.timeout(dt[1])\n",
    "    server.release(rqt)\n",
    "    \n",
    "\n",
    "def simulate():\n",
    "    N = []\n",
    "    env = simpy.Environment()\n",
    "    server = simpy.Resource(env, capacity=1)\n",
    "    env.process(arrivals(env, server, N))\n",
    "    env.run()\n",
    "    N = np.array(N)\n",
    "    E_W = (N + 1) / lmbda_2\n",
    "    return np.mean(E_W)\n",
    "\n",
    "n = 100\n",
    "T = [simulate() for i in range(0, n)]\n",
    "print(np.mean(T))\n",
    "print(np.std(T)/np.sqrt(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Stratification\n",
    "\n",
    "Note: suppose we want to calculate a confidence interval for our estimate of θ; we can estimate the variance of the estimator E as follows:\n",
    "\n",
    "1. Let $S_i^2$ be the sample variance for the samples of X∣Y=yi\n",
    "2. $\\sigma^2 = 1/n \\sum p_iS_i^2$ is an unbiased estimator for the variance of E\n",
    "\n",
    "Another reasonable question might be - if I have the choice between using Y and a control variate or as a stratifying variable, what is the better choice? It can, in fact, be shown that the stratified estimator will always lead to a greater (or at least equal to) variance reduction when compared to the control variate estimator for the same variable. There is a proof in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "n = 1000\n",
    "\n",
    "def h(x):\n",
    "    return x**2\n",
    "\n",
    "j = np.arange(0, n)\n",
    "\n",
    "U_j = (1 / n) * (uniform.rvs(size=n) + j)\n",
    "\n",
    "theta_hat = np.mean(h(U_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratification for Queueing Systems\n",
    "A focus in this course has been analyzing the outputs of discrete event simulations. These are often driven by an arrival process which has a known discrete distribution (for the number of arrivals by the end of the simulation N(T)). This would, of course, be a good candidate for the stratifying variable Y, as we have methods of generating arrivals time conditional on the number of arrivals, and then can run our system as usual given said arrival times. The issue, though, is that the distribution of this variable (typically Poisson) is not going to be have finite support. We can resolve this issue by stratifying on the following events: N(T)=j for  j=1,...,m and N(T)>m for some carefully chosen value of m.\n",
    "\n",
    "If N is a poisson process with intensity λ, then $N(T) \\sim Poisson(\\lambda T)$.\n",
    "\n",
    "If we want to sample the arrival times conditional on N(T)=j, then we can simply fix j, and generate the unordered arrival time steps, like we do in the ordered statistics method; this is equivalent to skipping generating a poisson random variable, and just plugging in the deterministic value for j. Likewise, to sample from N(T)>m, we can just sample poisson random variables, and reject them until we get a sample that satisfies the condition, and proceed with the ordered statistics method from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Stratification\n",
    "Post-stratification is a related technique. Suppose we have a discrete random variable Y that has a discrete support y1,...,yk with known probabilities pi=P(Y=yi). Perhaps we don't know how to sample from X∣Y=yi or it somehow otherwise difficult to do so. \n",
    "\n",
    "Y can be sampled alongside X though, i.e. every simulation of X also gives us a simulation of Y.\n",
    "\n",
    "Not having to estimate these quantities gives the post-stratification estimator an edge over the regular sample mean in terms of having less variance.\n",
    "\n",
    "It's typically much easier to do post-stratification if have a working implementation of our sampler; implement stratification directly often involves making changes to how the sampler is implemented, whereas post-stratification can be done simply on the output of the simulations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
