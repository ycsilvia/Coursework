{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1215629",
   "metadata": {},
   "source": [
    "<h2>Spark SQL and Spark Streaming</h2>\n",
    "\n",
    "<li>The file AAPL.csv contains price data for AAPL (Apple, Inc.). The data is one minute (time open high low close)</li>\n",
    "<li>The file stream_file contains streaming data with the same format</li>\n",
    "<li>Create factors from the static file and then use these factors with data from the streaming file to generate trading signals</li>\n",
    "<li>Follow the steps below. The difficult part of this assignment is that you're going to have to figure out how to use Window and lag for calculating percent changes and moving averages from the documentation/google/chatGPT. The rest should be fairly straightforward</li>\n",
    "\n",
    "Using this data, you should get the following streaming result:\n",
    "\n",
    "<pre>\n",
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+----------------+----------+--------------------+--------------------+------------------+\n",
    "|timestamp_stream| new_close|                 ma3|                 ma6|           r_close|\n",
    "+----------------+----------+--------------------+--------------------+------------------+\n",
    "|            1129|175.160004|0.005756422691690559|0.001445883402978...|0.7706769503643464|\n",
    "+----------------+----------+--------------------+--------------------+------------------+\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "//Create three windows (https://spark.apache.org/docs/3.5.0/api/scala/org/apache/spark/sql/expressions/Window$.html)\n",
    "val windowSpec1 =  //Window for 1 period\n",
    "val windowSpec3 =  //Window for three periods\n",
    "val windowSpec6=  //Window for 6 periods\n",
    "\n",
    "\n",
    "//Create case class for data input (the same case class will work for both static and streaming data)\n",
    "case class HistData()\n",
    "\n",
    "//Write a udf function remove_colon_udf that drops the colon from a string (e.g., \"9:30\" becomes \"930\")\n",
    "//(the Scala function replace will be useful here)\n",
    "\n",
    "val remove_colon_udf= \n",
    "\n",
    "/* Write a udf function add_one_minute_udf that adds one minute to a time. For example:\n",
    "930 becomes 931 (add 1)\n",
    "959 becomes 1000 (add 41)\n",
    "Don't worry about error checking\n",
    "\n",
    "You'll need Scala functions slice, trim, toInt\n",
    "Also, if you get a serializability error, figure out how to package the udf and the function as a unit\n",
    "*/\n",
    "\n",
    "def add_one_minute_udf = \n",
    "\n",
    "/*\n",
    "\n",
    "Read the static file into a dataframe and do calculations\n",
    "\n",
    "1. timestamp column contains date and time. Drop the date (ex: \"11/4/23 9:30\" becomes \"9:30\")\n",
    "2. using the previously written udf, remove the colon (\"9:30\" becomes \"930\")\n",
    "3. create a new column \"next_timestamp\" using the add_one_minute_udf on timestamp\n",
    "4. calculate r_close. \n",
    "   r_close = (close - open)/(high - low)\n",
    "   r_close indicates how the stock traded. If the close was the high and the open the low, the value is 1 \n",
    "   (i.e., the stock rose steadily over the minute). If the close is the low and the open the hight, the value is -1\n",
    "   (i.e., the stock tanked steadily over the minute)\n",
    "5. lag the price by 1 minute. \n",
    "    See: https://spark.apache.org/docs/3.5.0/api/java/org/apache/spark/sql/functions.html#lag-org.apache.spark.sql.Column-int-\n",
    "    (use lag(Column e,int offset))\n",
    "    Use windowSpec1 for this)\n",
    "    Call this column p_close (or previous_close)\n",
    "6. Calculate, pct_change, the one minute percent change (close/p_close - 1) \n",
    "7. Calculate ma3, the 3 minute moving average of pct_change \n",
    "    See the example\n",
    "8. Calculate ma6, the 6 minute moving average of pct_change\n",
    "9. select the columns next_timestamp, timestamp, r_close, ma3, ma6 and close\n",
    "    We'll use this for the join with the streaming dataframe\n",
    "    Name the resulting dataframe hist_data\n",
    "\n",
    "*/\n",
    "    \n",
    "val hist_data = \n",
    "\n",
    "/* The stream\n",
    "    * Streaming data is in the file stream_file\n",
    "    * Note that this is not kosher because the timestamps in the stream_file are also in hist_data\n",
    "    * In practice, they won't be (the future) but the assignment then becomes very complicated! \n",
    "\n",
    "1. Create a schema for the streaming data\n",
    "\n",
    "*/\n",
    "\n",
    "val liveStreamSchema = \n",
    "\n",
    "/*\n",
    "2. Manipulate the timestamp to change it to \"930\" rather than 11/4/23 9:30\n",
    "3. Rename the timestamp column (so that it won't clash with the timestamp column in hist_data)\n",
    "4. Rename the close column \n",
    "\n",
    "\n",
    "*/\n",
    "val liveDataStream = \n",
    "\n",
    "/* Join hist_data and liveDataStream and apply the trading rule (this is the query)\n",
    "\n",
    "4. Join hist_data and liveDataStream using next_timestamp from hist_data and the renamed timestamp from\n",
    "liveDataStream\n",
    "5. Apply the rules:\n",
    "    * ma3 > m6 (both are from hist_data)\n",
    "    * close <= new_close (close from hist_data, new_close from liveDataStream)\n",
    "    * r_close > 0 (from hist_data)\n",
    "6. Select the stream timestamp, new close, ma3, ma6 and r_close\n",
    "7. write the stream to the console\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "val liveStreamFactors = \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "liveStreamFactors.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ea1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
