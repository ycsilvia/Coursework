{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b285878",
   "metadata": {},
   "source": [
    "<h3>Spark combineByKey assignment</h3>\n",
    "In this assignment, you'll use combineByKey to find</li>\n",
    "<ul>\n",
    "    <li>the total number of taxi rides for each month in 2020</li>\n",
    "    <li>the average total taxi fare for each month in 2020</li>\n",
    "    <li>the standard deviation of the total fare for each month in 2020</li>\n",
    "    <li>draw a simple graph using this data</li>\n",
    "</ul>\n",
    "\n",
    "The data file is available at <a href=\"https://drive.google.com/file/d/1EMKVVp33U6_F3YOKOlnagqZ4RQlY2fjY/view?usp=drive_link\">https://drive.google.com/file/d/1EMKVVp33U6_F3YOKOlnagqZ4RQlY2fjY/view?usp=drive_link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90c9cf",
   "metadata": {},
   "source": [
    "<h3>Packages</h3>\n",
    "You're going to need several packages for the charting part of this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06504017",
   "metadata": {},
   "source": [
    "<h3>Run this cell if you're using Apache Torre on GCP</h3>\n",
    "<li>Note that this cell must be the first cell you run after loading the file or restarting the kernel</li>\n",
    "<li>The packages take some time to load. Wait for all of them to load before proceeding</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed96e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%AddDeps com.github.wookietreiber scala-chart_2.12 0.5.1\n",
    "%AddDeps org.jfree jfreechart 1.0.19\n",
    "%AddDeps org.scala-lang.modules scala-swing_2.12 3.0.0\n",
    "%AddDeps jfree jcommon 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2d261",
   "metadata": {},
   "source": [
    "<h3>Run this cell if you're using spylon kernel locally</h3>\n",
    "<li>Note that this cell must be the first cell you run after loading the file or restarting the kernel</li>\n",
    "<li>Note also that while you can use spylon kernel to test out your code, you must submit the GCP version of your assignment</li>\n",
    "<li>Be warned. It will be hard to deal with the 11 GB file on a local machine</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.num_executors = 4\n",
    "launcher.executor_cores = 2\n",
    "launcher.driver_memory = '4g'\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "launcher.packages = [\"com.github.wookietreiber:scala-chart_2.12:0.5.1\",\n",
    "                    \"org.jfree:jfreechart:1.0.19\",\n",
    "                    \"org.scala-lang.modules:scala-swing_2.12:3.0.0\",\n",
    "                    \"jfree:jcommon:1.0.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4545f",
   "metadata": {},
   "source": [
    "<h3>Place your entire code in the next cell</h3>\n",
    "<li>You can use the steps that follow to construct your code but <b>only the code in the next cell will be evaluated by the TAs!</b></li>\n",
    "<li>Submission requirements</li>\n",
    "<li><b>The notebook with the result (YYYYMM,(count, mean, stddev)) clearly visible in the output</b></li>\n",
    "<li><b>The graph with your cluster information</b></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9233cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "// YOUR ENTIRE CODE SHOULD BE IN THIS CELL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75377f0b",
   "metadata": {},
   "source": [
    "<h4>Read the data and construct (key,value) paired data</h4>\n",
    "<li>You need to give the full path to the file (gs://.....)</li>\n",
    "<li>We are interested in the pickup date (column 2) and the total amount (column 18)</li>\n",
    "<li>Since there are missing values (no total amount) and bad data (dates from 2018, 2003, etc.). Since we're only interested in 2017 data, your code should only return data from 2017. And, if there is any bad data, you need to get rid of it)</li>\n",
    "<li>Each line should contain 20 values. Because the split function in Scala omits trailing comma sequences (see example below), you need to count the number of commas in the input line string. There should be exactly 19</li>\n",
    "<li>write a function <span style=\"color:green\">extract_data</span> that returns either Some (good data) or None (bad data). Use flatMap to get rid of the bad data</li>\n",
    "<li>For example: </li>\n",
    "<pre>\n",
    "extractData(\"0,1,2018-01-01 00:32:05,2017-01-01 00:37:48,1,1.2,1,N,140,236,2,6.5,0.5,0.5,0.0,0.0,0.3,7.8,,\")\n",
    "</pre>\n",
    "should return None (2018 is the wrong year)\n",
    "<li>while:</li>\n",
    "<pre>\n",
    "extractData(\"0,1,2017-01-01 00:32:05,2017-01-01 00:37:48,1,1.2,1,N,140,236,2,6.5,0.5,0.5,0.0,0.0,0.3,7.8,,\")\n",
    "</pre>\n",
    "should return Some((201701,7.8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e232f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Scala skips trailing comma sequences\n",
    "\"1,1,,\".split(',') //should return Array(\"1\",\"1\",\"\") but returns Array(\"1\",\"1\")\n",
    "\n",
    "//The Scala count method takes a conditional boolean as an argument\n",
    "\"John Joe Bodega\".count(_ == 'o') //returns 3 (the count of the letter o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(line: String):Option[(String,Double)] = {\n",
    "    try {\n",
    "        if (line.count(_ ==',') != 19) None\n",
    "        else {\n",
    "            val array = line.split(\",\")\n",
    "            val tup = (array(2),array(17).toDouble)\n",
    "            val yyyy = tup._1.slice(0,4)\n",
    "            if (yyyy != \"2017\")\n",
    "                None\n",
    "            else {\n",
    "                val yyyymm = tup._1.slice(0,4) ++ tup._1.slice(5,7)\n",
    "                val final_data = (tup._1.slice(0,4) ++ tup._1.slice(5,7),tup._2)\n",
    "                Some(final_data)\n",
    "            }\n",
    "        }\n",
    "    } catch {\n",
    "        case e: Exception => None\n",
    "    }\n",
    "}\n",
    "extractData(\"0,1,2017-01-01 00:32:05,2017-01-01 00:37:48,1,1.2,1,N,140,236,2,6.5,0.5,0.5,0.0,0.0,0.3,7.8,,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0873ec3",
   "metadata": {},
   "source": [
    "<h3>Preapare the key,value pairs</h3>\n",
    "<li>read the data</li>\n",
    "<li>construct key value pairs using extractData</li>\n",
    "<li>data_rdd size: 113,500,159</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d81556",
   "metadata": {},
   "outputs": [],
   "source": [
    "//GCP bucket\n",
    "//val url = \"gs://hj2203-fall-2023/data/taxi_data_2017.csv\"\n",
    "//Local file\n",
    "val url = \"/Users/hardeepjohar/Downloads/tmp/taxi_data_2017.csv\"\n",
    "\n",
    "val data_rdd = sc.textFile(url)\n",
    "    .flatMap(r => extractData(r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43e39d",
   "metadata": {},
   "source": [
    "<h3>Write the combiner, merger, mergeAndCombiner and getAvgAndStd functions</h3>\n",
    "<li>Think about what goes into each and how you should accumulate values</li>\n",
    "<li>For standard deviation, use the formula below (though accuracy is not guaranteed!)</li>\n",
    "<li>This makes it possible to calculate mean and square root with only one pass through the data</li>\n",
    "<p>\n",
    "    </p>\n",
    "$ \\sqrt{\\frac{1}{N}\\Sigma x_{i}^{2}-\\mu^{2}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d84c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "val combiner = (x: Double) => (1,x,x*x)\n",
    "val merger = (x: (Int, Double, Double),y: Double) => {\n",
    "    val (c,acc_sum,acc_squares) = x\n",
    "    (c+1, acc_sum + y, acc_squares + y*y)\n",
    "}\n",
    "val mergeAndCombiner = (x1: (Int, Double, Double), x2: (Int, Double, Double)) => {\n",
    "    val (c1,acc_sum1,acc_squares1) = x1\n",
    "    val (c2,acc_sum2,acc_squares2) = x2\n",
    "    (c1+c2,acc_sum1+acc_sum2,acc_squares1 + acc_squares2)\n",
    "}\n",
    "\n",
    "val getAvgAndStd = (x: (String, (Int, Double, Double))) => {\n",
    "    val (identifier, (count,acc_sum,acc_squares)) = x\n",
    "    val mean = acc_sum/count.toDouble\n",
    "    val stdev = math.sqrt(1.0/count.toDouble*acc_squares - acc_sum*acc_sum/(count.toDouble*count.toDouble))\n",
    "    (identifier,(count,mean,stdev))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = data_rdd.combineByKey(combiner,merger,mergeAndCombiner).map(getAvgAndStd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217361fb",
   "metadata": {},
   "source": [
    "<h3>Get the result in a scala Array</h3>\n",
    "<li>There should be 12 lines of data, 201701, 201702, ...,201712</li>\n",
    "<li>Sort the data in order of month (see https://alvinalexander.com/scala/how-to-sort-map-in-scala-key-value-sortby-sortwith/)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9438fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val scala_result = result.collect.sortBy(_._1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_result.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adcee90",
   "metadata": {},
   "source": [
    "<h3>Draw a bar chart comparing number of monthly rides</h3>\n",
    "<li>Save it to a file (use the location /tmp/rides.png)</li>\n",
    "<li>Do necessary imports and see the example below</li>\n",
    "<li>To access the saved file, in the Jupyter file navigator, go to /Local Disk/tmp and look for this file. Note that the file will disappear when you delete the cluster!</li>\n",
    "<li>Open the file in your browser and screenshot it. Make sure that your cluster information is included in the screenshot</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scalax.chart.api._\n",
    "import org.jfree._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7feb28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2aaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//data is a Scala Vector object. To convert an Array to a Vector, use .toVector\n",
    "val data = for (i <- 1 to 5) yield (i,i*i)\n",
    "val chart = XYBarChart(data)\n",
    "//val chart = XYBarChart(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5536a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b031d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = scala_result.map(t => (t._1.toInt,t._2._1)).toVector\n",
    "val chart = XYBarChart(data)\n",
    "chart.saveAsPNG(\"sample_chart.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af21acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
